Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rappoport2018,
abstract = {Recent high throughput experimental methods have been used to collect large biomedical omics datasets. Clustering of single omic datasets has proven invaluable for biological and medical research. The decreasing cost and development of additional high throughput methods now enable measurement of multi-omic data. Clustering multi-omic data has the potential to reveal further systems-level insights, but raises computational and biological challenges. Here, we review algorithms for multi-omics clustering, and discuss key issues in applying these algorithms. Our review covers methods developed specifically for omic data as well as generic multi-view methods developed in the machine learning community for joint clustering of multiple data types. In addition, using cancer data from TCGA, we perform an extensive benchmark spanning ten different cancer types, providing the first systematic comparison of leading multi-omics and multi-view clustering algorithms. The results highlight key issues regarding the use of single- versus multi-omics, the choice of clustering strategy, the power of generic multi-view methods and the use of approximated p-values for gauging solution quality. Due to the growing use of multi-omics data, we expect these issues to be important for future progress in the field.},
author = {Rappoport, Nimrod and Shamir, Ron},
doi = {10.1093/nar/gky889},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rappoport, Shamir - 2018 - Multi-omic and multi-view clustering algorithms review and cancer benchmark.pdf:pdf},
issn = {0305-1048},
journal = {Nucleic Acids Research},
number = {20},
pages = {10546--10562},
pmid = {30295871},
publisher = {Oxford University Press},
title = {{Multi-omic and multi-view clustering algorithms: review and cancer benchmark}},
volume = {46},
year = {2018}
}
@article{Karim2021,
abstract = {Clustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.},
author = {Karim, Md Rezaul and Beyan, Oya and Zappa, Achille and Costa, Ivan G. and Rebholz-Schuhmann, Dietrich and Cochez, Michael and Decker, Stefan},
doi = {10.1093/bib/bbz170},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karim et al. - 2021 - Deep learning-based clustering approaches for bioinformatics.pdf:pdf},
issn = {14774054},
journal = {Briefings in Bioinformatics},
number = {1},
pages = {393--415},
pmid = {32008043},
title = {{Deep learning-based clustering approaches for bioinformatics}},
volume = {22},
year = {2021}
}
@article{Makhzani2015,
abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
archivePrefix = {arXiv},
arxivId = {1511.05644},
author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
doi = {10.48550/arxiv.1511.05644},
eprint = {1511.05644},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Makhzani et al. - 2015 - Adversarial Autoencoders.pdf:pdf},
month = {nov},
title = {{Adversarial Autoencoders}},
url = {https://arxiv.org/abs/1511.05644v2 http://arxiv.org/abs/1511.05644},
year = {2015}
}
@article{Kingma2019,
abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
archivePrefix = {arXiv},
arxivId = {1906.02691},
author = {Kingma, Diederik P. and Welling, Max},
doi = {10.1561/2200000056},
eprint = {1906.02691},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Welling - 2019 - An introduction to variational autoencoders.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
number = {4},
pages = {307--392},
title = {{An introduction to variational autoencoders}},
volume = {12},
year = {2019}
}
@incollection{Masood2015,
abstract = {Dealing with data means to group information into a set of categories either in order to learn new artifacts or understand new domains. For this purpose researchers have always looked for the hidden patterns in data that can be defined and compared with other known notions based on the similarity or dissimilarity of their attributes according to well-defined rules. Data mining, having the tools of data classification and data clustering, is one of the most powerful techniques to deal with data in such a manner that it can help researchers identify the required information. As a step forward to address this challenge, experts have utilized clustering techniques as a mean of exploring hidden structure and patterns in underlying data. Improved stability, robustness and accuracy of unsupervised data classification in many fields including pattern recognition, machine learning, information retrieval, image analysis and bioinformatics, clustering has proven itself as a reliable tool. To identify the clusters in datasets algorithm are utilized to partition data set into several groups based on the similarity within a group. There is no specific clustering algorithm, but various algorithms are utilized based on domain of data that constitutes a cluster and the level of efficiency required. Clustering techniques are categorized based upon different approaches. This paper is a survey of few clustering techniques out of many in data mining. For the purpose five of the most common clustering techniques out of many have been discussed. The clustering techniques which have been surveyed are: K-medoids, K-means, Fuzzy C-means, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Self-Organizing Map (SOM) clustering.},
author = {Masood, Muhammad Ali and Khan, M N A and Zulfikar, Shaheed and Bhutto, Ali},
booktitle = {Modern Education and Computer Science},
doi = {10.5815/ijmecs.2015.01.06},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Masood et al. - 2015 - Modern Education and Computer Science.pdf:pdf},
keywords = {DBSCAN,Data Mining,Hierarchical Clustering,Index Terms-Clustering Techniques,Performance Analysis},
pages = {38--46},
title = {{Clustering Techniques in Bioinformatics}},
url = {http://www.mecs-press.org/},
volume = {1},
year = {2015}
}
@article{Fortuin2021,
abstract = {Clustering high-dimensional data, such as images or biological measurements, is a longstanding problem and has been studied extensively. Recently, Deep Clustering has gained popularity due to its flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of highdimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the lower dimensional latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on the MNIST benchmark data set and challenging real-world tasks of clustering mouse organs from single-cell RNA-sequencing measurements and defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines as well as competitor methods.},
author = {Fortuin, Vincent and Somnath, Vignesh Ram and Claassen, Manfred},
doi = {10.1371/journal.pcbi.1009086},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fortuin, Somnath, Claassen - 2021 - Mixture-of-Experts Variational Autoencoder for clustering and generating from similaritybased repres.pdf:pdf},
isbn = {1111111111},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {6},
pages = {1--17},
pmid = {34191792},
title = {{Mixture-of-Experts Variational Autoencoder for clustering and generating from similaritybased representations on single cell data Andreas KopfID}},
url = {http://dx.doi.org/10.1371/journal.pcbi.1009086},
volume = {17},
year = {2021}
}
@book{Ketkar2021,
abstract = {{\textcopyright} Springer-Verlag London 2014. All rights are reserved. Providing a broad but in-depth introduction to neural network and machine learning in a statistical framework, this book provides a single, comprehensive resource for study and further research. All the major popular neural network models and statistical learning approaches are covered with examples and exercises in every chapter to develop a practical working understanding of the content. Each of the twenty-five chapters includes state-of-the-art descriptions and important research results on the respective topics. The broad coverage includes the multilayer perceptron, the Hopfield network, associative memory models, clustering models and algorithms, the radial basis function network, recurrent neural networks, principal component analysis, nonnegative matrix factorization, independent component analysis, discriminant analysis, support vector machines, kernel methods, reinforcement learning, probabilistic and Bayesian networks, data fusion and ensemble learning, fuzzy sets and logic, neurofuzzy models, hardware implementations, and some machine learning topics. Applications to biometric/bioinformatics and data mining are also included. Focusing on the prominent accomplishments and their practical aspects, academic and technical staff, graduate students and researchers will find that this provides a solid foundation and encompassing reference for the fields of neural networks, pattern recognition, signal processing, machine learning, computational intelligence, and data mining.},
author = {Ketkar, Nikhil and Moolayil, Jojo},
booktitle = {Deep Learning with Python},
doi = {10.1007/978-1-4842-5364-9},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ketkar, Moolayil - 2021 - Deep Learning with Python.pdf:pdf},
isbn = {9781617296864},
title = {{Deep Learning with Python}},
year = {2021}
}
@article{Min2018,
abstract = {Clustering is a fundamental problem in many data-driven application domains, and clustering performance highly depends on the quality of data representation. Hence, linear or non-linear feature transformations have been extensively used to learn a better data representation for clustering. In recent years, a lot of works focused on using deep neural networks to learn a clustering-friendly representation, resulting in a significant increase of clustering performance. In this paper, we give a systematic survey of clustering with deep learning in views of architecture. Specifically, we first introduce the preliminary knowledge for better understanding of this field. Then, a taxonomy of clustering with deep learning is proposed and some representative methods are introduced. Finally, we propose some interesting future opportunities of clustering with deep learning and give some conclusion remarks.},
author = {Min, Erxue and Guo, Xifeng and Liu, Qiang and Zhang, Gen and Cui, Jianjing and Long, Jun},
doi = {10.1109/ACCESS.2018.2855437},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Min et al. - 2018 - A Survey of Clustering with Deep Learning From the Perspective of Network Architecture.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Clustering,data representation,deep learning,network architecture},
pages = {39501--39514},
publisher = {IEEE},
title = {{A Survey of Clustering with Deep Learning: From the Perspective of Network Architecture}},
volume = {6},
year = {2018}
}
@article{Palacio-Nino2019,
abstract = {Determining the quality of the results obtained by clustering techniques is a key issue in unsupervised machine learning. Many authors have discussed the desirable features of good clustering algorithms. However, Jon Kleinberg established an impossibility theorem for clustering. As a consequence, a wealth of studies have proposed techniques to evaluate the quality of clustering results depending on the characteristics of the clustering problem and the algorithmic technique employed to cluster data.},
archivePrefix = {arXiv},
arxivId = {1905.05667},
author = {Palacio-Ni{\~{n}}o, Julio-Omar and Berzal, Fernando},
eprint = {1905.05667},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palacio-Ni{\~{n}}o, Berzal - 2019 - Evaluation Metrics for Unsupervised Learning Algorithms.pdf:pdf},
title = {{Evaluation Metrics for Unsupervised Learning Algorithms}},
url = {http://arxiv.org/abs/1905.05667},
year = {2019}
}
@article{Xie2015,
abstract = {Clustering is central to many data-driven application domains and has been
studied extensively in terms of distance functions and grouping algorithms.
Relatively little work has focused on learning representations for clustering.
In this paper, we propose Deep Embedded Clustering (DEC), a method that
simultaneously learns feature representations and cluster assignments using
deep neural networks. DEC learns a mapping from the data space to a
lower-dimensional feature space in which it iteratively optimizes a clustering
objective. Our experimental evaluations on image and text corpora show
significant improvement over state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1511.06335},
author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
doi = {10.48550/arxiv.1511.06335},
eprint = {1511.06335},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie, Girshick, Farhadi - 2015 - Unsupervised Deep Embedding for Clustering Analysis.pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
month = {nov},
pages = {740--749},
publisher = {International Machine Learning Society (IMLS)},
title = {{Unsupervised Deep Embedding for Clustering Analysis}},
url = {https://arxiv.org/abs/1511.06335v2},
volume = {1},
year = {2015}
}
@article{Blekherman2011,
abstract = {It is well known that significant metabolic change take place as cells are transformed from normal to malignant. This review focuses on the use of different bioinformatics tools in cancer metabolomics studies. The article begins by describing different metabolomics technologies and data generation techniques. Overview of the data pre-processing techniques is provided and multivariate data analysis techniques are discussed and illustrated with case studies, including principal component analysis, clustering techniques, self-organizing maps, partial least squares, and discriminant function analysis. Also included is a discussion of available software packages. {\textcopyright} 2011 The Author(s).},
author = {Blekherman, Grigoriy and Laubenbacher, Reinhard and Cortes, Diego F. and Mendes, Pedro and Torti, Frank M. and Akman, Steven and Torti, Suzy V. and Shulaev, Vladimir},
doi = {10.1007/s11306-010-0270-3},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blekherman et al. - 2011 - Bioinformatics tools for cancer metabolomics.pdf:pdf},
issn = {15733882},
journal = {Metabolomics},
keywords = {Bioinformatics,Cancer,Mass spectrometry,Metabolite profiling,Metabolomics,NMR},
month = {sep},
number = {3},
pages = {329--343},
publisher = {Springer},
title = {{Bioinformatics tools for cancer metabolomics}},
url = {https://link.springer.com/article/10.1007/s11306-010-0270-3},
volume = {7},
year = {2011}
}
