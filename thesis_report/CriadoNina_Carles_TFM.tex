% !TeX encoding = UTF-8
% !TeX spellcheck = ca_ES
\documentclass[CAT,BIB]{TFUOC}%IB: CASTELLÀ, CAT: CATALÀ, ENG: ANGLÈS

%Begin Carles customization{

    %Load packages
        %Modifies ToC: include References, remove LoF, LoT
        \usepackage[nottoc, notlof, notlot, numbib]{tocbibind}

%        %Insert page number in reference
%        \usepackage{varioref}

        %Convert references into links
        \usepackage{hyperref}
        \hypersetup{
            colorlinks=true,
            allcolors=darkblueUOC,
            pdfauthor=Carles Criado Ninà,
        }

        %Use acronyms
        \usepackage[acronym,nomain,section=section,numberedsection=autolabel]{glossaries}
            % Make glossary links black
            \renewcommand*{\glstextformat}[1]{\textcolor{black}{#1}}
            % Generate the glossary
            \makeglossaries
            % Define acronyms:
            \newacronym{tfm}{TFM}{treball de final de màster}

            \newacronym{cq}{CQ}{qualitat de \textit{clustering} (\textit{clustering quality})}
            \newacronym{fl}{FL}{aprenentatge de característiques \textit{(feature learning)}}
            \newacronym{rl}{RL}{capa de representació \textit{(representation layer)}}
            \newacronym{lf}{LF}{característiques apreses \textit{(learned features)}}

            \newacronym{pca}{PCA}{anàlisi de components principals}
            \newacronym{kmeans}{K-means}{K-means}
            \glsunset{kmeans}
            \newacronym{gmm}{GMM}{model de barreja de gaussianes}
            \newacronym{aglo}{Aglo.}{model aglomeratiu}

            \newacronym{mlp}{MLP}{\textit{multilayer perceptron}}
            \newacronym{cnn}{CNN}{\textit{convolutional neural network}}
            \newacronym{dbn}{DBN}{\textit{deep belief network}}
            \newacronym{gan}{GAN}{\textit{generative adversarial network}}
            \newacronym{lstm}{LSTM}{\textit{long short-term memory}}
            \newacronym{ae}{AE}{\textit{autoencoder}}
            \newacronym{cae}{CAE}{\textit{convolutional autoencoder}}
            \newacronym{vae}{VAE}{\textit{variational autoencoder}}
            \newacronym{lstmae}{LSTM-AE}{\textit{long short-term memory autoencoder}}
            \newacronym{aae}{AAE}{\textit{adversarial autoencoder}}
            \newacronym{dae}{DAE}{\textit{denoising autoencoder}}
            \newacronym{sae}{SAE}{\textit{stacked autoencoder}}
            \newacronym{dec}{DEC}{Deep embedded clustering}
            \newacronym{vade}{VaDE}{Variational Deep Embedding}

            \newacronym{relu}{ReLU}{unitat lineal rectificada, (\textit{rectified linear unit})}
            \glsunset{relu}
            \newacronym{elbo}{ELBO}{límit inferior de la evidència (\textit{evidence lower bound})}
            \newacronym{kl}{KL}{Kullback-Leibler}
            \newacronym{mnist}{MNIST}{\textit{Modified National Institute of Standards and Technology database}}


        %Insert names of references with \cref{}
        \usepackage[catalan,noabbrev,nameinlink]{cleveref}
            %Use with catalan language: replace "gràfic" with "figura":
            \crefname{figure}{figura}{figures}
            \Crefname{figure}{Figura}{Figures}

        %Use \citet{} to citate inline, or \citep{} to reference between brackets
        \usepackage[square,numbers,sort&compress]{natbib}

        %Use to insert text boxes. Example:
        %    \begin{tcolorbox}[title=Títol, colback=red!5!white, colframe=red!50!black]
        %        Contingut...
        %    \end{tcolorbox}
        \usepackage{tikz, tcolorbox}
        \newcommand{\todo}[1]{
            \begin{tcolorbox}[title=ToDo!, colback=red!5!white, colframe=red!50!black, coltext=red!50!black]
            #1
            \end{tcolorbox}}

        %Provides \mathbb{}, necessary for some maths symbols
        \usepackage{amsfonts}

        %Use small font for image captions
        \usepackage[font=footnotesize,labelfont=bf]{caption}

%        %Insert sideways figure
%        \usepackage{rotating}

%        %Insert landscape page
%        \usepackage{lscape}

        % Tables with merged rows/columns
        \usepackage{multirow}





    %Force all nested enumerate lists to be numbered
    \renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}}
    \renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}}
    \renewcommand{\labelenumiv}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.\arabic{enumiv}}

%}End Carles customization

%Introducció de dades del treball
\title{Implemenació de models de \textit{deep clustering} en dades metabolòmiques.}
\titcrt{\textit{Deep clustering} en dades metabolòmiques} %Títol curt que apareixerà a la capçalera
\author{Carles Criado Ninà}
\date{15 de gener de 2023}


\nomPDC{Esteban Vegas Lozano}
\nomPRA{Carles Ventura Royo}
\titulac{Màster en Bioinformàtica i Bioestadística}
\area{Àrea 3}
\idioma{Català}
\credits{15}
\parcla{clustering, deep learning, autoencoder, metabolomics}

\licenc{ccBySa}
%Possibles llicències
%ccByNcNd
%ccByNcSa
%ccByNc
%ccByNd
%ccBySa
%ccBy
%GNU
%copyright


%%%%%%%%%%
% Resum en l'idioma

\abstractidioma{
Màxim 250 paraules, amb la finalitat, context d’aplicació, metodologia, resultats i conclusions del treball.
}

% Resum en anglès.
\abstractenglish{
A maximum of 250 words, detailing the purpose, context of application, methodology, results and conclusions of the work.
}

\begin{document}

\estructura

\tableofcontents

\listoffigures

\listoftables




\chapter{Introducció}
    %Aquesta plantilla es concep com una guia per a l’estudiant. Es pot adaptar a les necessitats de cada treball, sempre que el/la tutor/a del treball hi estigui d’acord.

\todo{Actualitzar tot segons estat final del projecte (aprofitar informes PAC2 i 3).}

    \section{Context i justificació del treball}
        %Punt de partida del treball (Quina és la necessitat a cobrir? Per què és un tema rellevant? Com es resol el problema de moment?) i aportació realitzada (Quin resultat es vol obtenir?).
        %
        %És important tenir en compte que el treball final ha de ser comprensible per a qualsevol persona que conegui l'àrea de coneixement, però no té perquè ser experta en el tema del que versa el treball.

        Una de les problemàtiques característiques del camp de la bioinformàtica és que les dades que s'estudien habitualment contenen un nombre molt elevat de variables, però un nombre reduït de mostres. Aquesta elevada dimensionalitat de les dades suposa una dificultat a l'hora de realitzar estudis estadístics.

        Un dels conjunts de tècniques estadístiques utilitzades en dades bioinformàtiques són els estudis de \textit{clustering}. Aquestes tècniques es basen en agrupar les dades en funció de la seva similitud i permeten extreure informació del conjunt de les dades, com per exemple la seva estructura latent \citep{Karim2021, Masood2015}.

        Les tècniques de \textit{clustering} clàssiques no funcionen bé en dades amb una elevada dimensionalitat, i això suposa una limitació en l'estudi de dades bioinformàtiques. Una solució alternativa que s'utilitza habitualment per combatre aquest problema és la d'aplicar primer una tècnica de reducció de la dimensionalitat, com per exemple l'anàlisi de components principals, i aplicar posteriorment les tècniques de \textit{clustering} sobre les dades transformades \citep{Min2018, Masood2015}.

        Un dels avantatges de les xarxes neuronals és que admeten treballar amb dades amb una dimensionalitat elevada. Això ha fet possible desenvolupar un gran nombre de mètodes de \textit{clustering} basats en \textit{deep learning}, que es poden dividir en dos grans grups: mètodes basats reduir la dimensionalitat i aplicar mètodes de \textit{clustering} sobre les dades transformades; i mètodes que desenvolupen un model de \textit{clustering} sobre les dades originals sense transformar \citep{Karim2021}.

        En aquest estudi es pretén desenvolupar un model de \textit{deep clustering} basat en l'arquitectura \textit{Variational Autoencoder}, un tipus de model generatiu que es basa en mètodes probabilístics Bayesians \citep{Kingma2019}. El model resultant s'aplicarà sobre un conjunt de dades metabolòmiques obtingut d'un anàlisi clínic i es valorarà la interpretabilitat del resultat en funció de la relació entre els clústers generats i els grups clínics diferenciats en l'estudi.

    \section{Objectius del treball}
    \label{s:objectius}
        %Llistat dels objectius del treball.

        \begin{enumerate}
            \item Desenvolupar un mètode de \textit{deep clustering} basat un model de \textit{Variational autoencoder}, aplicat a un conjunt de dades metabolòmiques.
            \begin{enumerate}
                \item Contextualitzar el problema a resoldre realitzant una extensiva recerca bibliogràfica.
                \item Desenvolupar un model de \textit{deep clustering} utilitzant un \textit{variational autoencoder}.
                \item Aplicar el model desenvolupat a un conjunt de dades metabolòmiques i estudiar els resultats.
            \end{enumerate}
        \end{enumerate}

    \section{Impacte en sostenibilitat, ètic-social i de diversitat}
    \label{s:etic}
        %Aquesta secció hauria d'identificar els impactes positius i/o negatius del treball final en les tres dimensions de la competència transversal UOC “Compromís ètic i global”.
        %La Guia transversal sobre la Competència Ètica i Global us ajudarà a redactar aquests apartats.


    \section{Enfocament i mètode seguit}
        %Menció de quines són les possibles estratègies per dur a terme el treball i quina és l’estratègia triada (desenvolupar un producte nou, adaptar un producte existent…). Cal incloure una valoració de per què aquesta és l’estratègia més apropiada per aconseguir els objectius.


    \section{Planificació del treball}
        %Descripció dels recursos necessaris per fer el treball, les tasques a realitzar i una planificació temporal de cada tasca mitjançant un diagrama de Gantt o similar. Aquesta planificació hauria de marcar quins són les fites parcials de cadascuna de les PAC.

        \subsection{Tasques:}
        \label{sec:tasques}

            A continuació es llisten les tasques a realitzar per la consecució dels objectius definits a la \cref{sec:objectius_especifics}. Al diagrama de Gantt de la \cref{fig:gantt} es reflecteix el temps assignat a cada tasca.

            \begin{enumerate}
                \item %Objectiu 1
                \begin{enumerate}
                    \item Caracteritzar les tècniques clàssiques de \textit{clustering} aplicades a dades metabolòmiques.
                    \item Caracteritzar les tècniques de \textit{clustering} basades en \textit{deep learning}.
                    \item Estudiar en profunditat l’arquitectura de \textit{variational autoencoder} i com s’aplica en \textit{clustering}.
                    \item Estudiar el funcionament de Keras, practicar utilitzant exemples.
                \end{enumerate}

                \item %Objectiu 2
                \begin{enumerate}
                    \item Obtenir un conjunt de dades adequat per a realitzar l'estudi.
                    \item Realitzant un estudi preliminar amb tècniques clàssiques que servirà com a marc de referència.
                    \item Desenvolupar un model de \textit{deep clustering}.
                    \item Comparar els resultats obtinguts amb el marc de referència.
                \end{enumerate}

                \item %Objectiu 3
                \begin{enumerate}
                    \item Aplicar el model a un conjunt de dades metabolòmiques.
                    \item Estudiar la interpretabilitat dels resultats comparant els clústers generats amb els grups reals de les dades.
                \end{enumerate}
            \end{enumerate}

        \subsection{Calendari:}
        \label{sec:calendari}

            A la \cref{fig:gantt} es mostra un diagrama de Gantt on es representa el temps assignat a cada tasca, així com a altres activitats relacionades amb el TFM com són la redacció dels informes de les diferents PACs, l'elaboració de la memòria i de la presentació i la preparació de la defensa.

            En la planificació s'ha tingut en compte el pla docent i els dies festius. S'ha calculat una dedicació de 4 hores per dia laborable, encara que s'ha contemplat la possibilitat de dedicar temps addicional puntualment per assegurar el compliment de les fites. En total, es preveu una dedicació de 312 hores.

            \begin{figure}[htpb]
                \centering
                \includegraphics[width=\textwidth]{TFM_calendari.png}
                \caption{Diagrama de Gantt.}
                \label{fig:gantt}
            \end{figure}

        \subsection{Fites:}
        \label{sec:fites}

            S'han definit les següents fites com a dates clau per planificar el treball:

            \begin{description}
                \item[17/10/2022] Elaboració del pla de treball. Entrega de l'informe de la PAC 1.
                \item[28/10/2022] Finalitzar la recerca bibliogràfica (marc teòric).
                \item[04/11/2022] Data límit per a l'obtenció d'un conjunt de dades.
                Consecució de l'objectiu 1.
                \item[21/11/2022] Entrega de l'informe de la PAC 2.
                \item[09/12/2022] Consecució de l'objectiu 2.
                \item[16/12/2022] Consecució de l'objectiu 3.
                \item[24/12/2022] Entrega de l'informe de la PAC 3.
                \item[15/01/2023] Entrega de la memòria.
                \item[23/01/2023] Presentació i defensa preparades.
            \end{description}


    \section{Breu sumari de productes obtinguts}
        %No cal entrar en detall: la descripció detallada es farà a la resta de capítols.

    \section{Breu descripció dels altres capítols de la memòria}
        %Breu explicació dels continguts de cada capítol i la seva relació amb el projecte global.

\chapter{Estat de l'art}
\label{s:state}
    %Estat de l'art del tema en qüestió.
    %Hauria d'acabar mostrant per què el treball és important i aporta alguna cosa, i amb les hipòtesis del treball.

\todo{Falta incorporar moltes referències, sobretot a les \cref{s:state_reduccio,s:state_ae}.}

    \section{Metabolòmica}
    \label{s:state_metabol}

\todo{
    Pendent de definir el tipus de dades que treballarem. En principi esperem que siguin dades metabolòmiques. En aquest secció farem un molt breu resum (2-3 paràgrafs) de la tècnica i el tipus de dades obtingudes. Finalitzarem mencionant l'objectiu d'aplicar \textit{clustering}, per connectar-ho amb la següent secció.
}

    \section{Tècniques de \textit{clustering}}
    \label{s:state_cluster}

        Les tècniques de \textit{clustering} són un conjunt de tècniques d'aprenentatge automàtic no supervisat que es basen en agrupar les observacions en funció d'alguna mesura de la seva similitud, sense coneixement previ de l'estructura de les observacions o el nombre de grups que es pretén obtenir.

        L'objectiu d'aquestes tècniques és separar les dades en diversos grups que siguin internament homogenis (les observacions dins el mateix grup són similars entre ells) i tinguin característiques diferents a la resta de grups.

        Identificar aquests grups permet obtenir informació sobre l'estructura de les dades com la presència de patrons, i pot servir com a un punt de partida en l'exploració de les dades. Per exemple, aplicades a dades metabolòmiques, les tècniques de \textit{clustering} poden permetre identificar diferents tipus cel·lulars en funció de la seva expressió metabòlica \citep{Blekherman2011}.

        Existeix una gran varietat de tècniques de \textit{clustering} \citep{Karim2021, Min2018, Blekherman2011, Masood2015}. Encara que totes comparteixen un mateix objectiu, aborden el problema aplicant diferents criteris i per tant els grups que formen poden no coincidir. Algunes de les aproximacions més utilitzades històricament es basen en mètodes matemàtics i estadístics \citep{Masood2015}, com mètodes jeràrquics, mètodes basats en centroides, distribucions o densitats \citep{Karim2021}.

        \textbf{Mètodes jeràrquics:} es basen en crear clústers amb un ordre predeterminat, on els clústers de més baix nivell es combinen iterativament per crear clústers més grans. Això els dota d'una estructura jeràrquica que es pot presentar en forma de dendrograma, però implica que l'assignació de cada punt a un clúster és determinista. Aquests algoritmes són sensibles al soroll \citep{Karim2021}.

        \textbf{Mètodes basats en centroides:} (p. ex. \textit{k-means}) es basen assignar cada punt a un número predeterminat de grups i calcular els seus centroides. Després es canvia iterativament l'assignació de cada punt als diferents grups fins a minimitzar la suma de les distàncies de cada punt al centroide del seu grup. En general aconsegueixen un millor rendiment que els mètodes jeràrquics, però son incapaços de trobar grups no convexos \citep{Karim2021}.

        \textbf{Mètodes basats en distribucions:} (p. ex. model de barreja gaussiana (GMM)) es basen en modelar els grups en funció d'una barreja de distribucions probabilístiques. La seva base estadística permet inferir relacions entre les característiques de les dades, però requereix de fortes assumpcions sobre la distribució de les dades i tenen tendència a sobre-ajustament \citep{Karim2021}.

        \textbf{Mètodes basats en densitats:} (p. ex. DBSCAN) defineixen els clústers com àrees amb major densitat comparats amb la resta de les dades. Els punts en regions més disperses es consideren fronteres o soroll. El punt negatiu d'aquests mètodes és precisament que requereixen d'una disminució de la densitat per detectar les fronteres dels clústers i són poc eficaces en separar grups contigus \citep{Karim2021}.

    \section{Reducció de la dimensionalitat}
    \label{s:state_reduccio}

        Un dels problemes que apareix habitualment al analitzar dades bioinformàtiques és la seva elevada dimensionalitat, característica que fa que les tècniques de \textit{clustering} clàssiques no funcionin bé \citep{Masood2015, Karim2021}. Per combatre aquest problema, una solució és aplicar tècniques de reducció de la dimensionalitat i posteriorment aplicar les tècniques de \textit{clustering} sobre les dades transformades \citep{Min2018, Masood2015}.

        La reducció de la dimensionalitat s'aconsegueix mitjançant tècniques d'\gls{fl}. L'objectiu d'aquestes tècniques és representar les dades originals en un espai dimensional inferior, aplicant una transformació que retingui el màxim d'informació. L'espai reduït resultant s'anomena \gls{rl} i les seves dimensions \gls{lf}.

        Històricament s'han utilitzat tècniques matemàtiques per realitzar \gls{fl}, que poden ser transformacions lineals (anàlisi de components principals) o no lineals (mètodes kernel o tècniques espectrals). Posteriorment s'apliquen les tècniques de clustering sobre les \gls{lf}.

        Una limitació dels mètodes lineals és que no són capaços de retenir informació sobre relacions no lineals en les dades, el que provoca una reducció de la \gls{cq}. Les tècniques no lineals són més adequades \citep{Karim2021}.

    \section{Tècniques de \textit{deep clustering}}
    \label{s:state_deep}

        Les xarxes neuronals profundes possibiliten aplicar tècniques de \gls{fl} més eficients que les descrites a la secció anterior, aplicant mètodes no lineals complexes que permeten capturar \gls{lf} més rellevants. En particular, la funció de transformació es pot optimitzar mitjançant l'aprenentatge dels paràmetres de la xarxa, el que permet extreure \gls{lf} òptimes per obtenir una bona \gls{cq} \citep{Karim2021}.

        La seva recent popularització ha fet possible desenvolupar un gran nombre de tècniques de \textit{clustering} basades en \textit{deep learning}, anomenades en conjunt tècniques de \textit{deep clustering}. Es poden dividir en dos grans grups: mètodes de dos passos, que es basen en utilitzar tècniques de \gls{fl} i aplicar mètodes de \textit{clustering} convencionals sobre les \gls{lf}; i mètodes d'un sol pas que desenvolupen un model de \textit{clustering} sobre les dades originals \citep{Karim2021}.

        A la literatura s'ha descrit una gran varietat de mètodes basats en diferents arquitectures neuronals (\gls{mlp}, \gls{cnn}, \gls{dbn}, \gls{gan}, \gls{ae}). A \url{https://github.com/rezacsedu/Deep-learning-for-clustering-in-bioinformatics} \citep{Karim2021} es pot trobar un llistat amb enllaços als articles originals.

        La majoria d'aproximacions actuals utilitzen una arquitectura basada en un \gls{ae} \citep{Karim2021}, donat que és capaç d'obtenir \gls{lf} eficients per realitzar \textit{clustering}.


    \section{Autoencoder}
    \label{s:state_ae}

        L'arquitectura \gls{ae} està formada per dues parts: un codificador i un descodificador. La funció del codificador és representar l'entrada en un espai dimensional reduït, sovint un vector, anomenat codi. La funció del descodificador és reconstruir l'entrada original a partir del codi.

        Optimitzant els paràmetres d'ambdues parts conjuntament durant l'entrenament de la xarxa es busca aconseguir que el codificador sigui capaç de comprimir la entrada mantenint el màxim d'informació. Aplicant la idea de \gls{fl}, el codi esdevé la \gls{rl}.

        Per aconseguir un \textit{clustering} eficaç, no és suficient amb aquesta optimització ja que si no s'aplica cap restricció a la \gls{rl} podria donar lloc a \gls{lf} que no permeten una bona \gls{cq}. Per resoldre aquest problema es defineixen dos tipus de funció de cost, que s'optimitzen conjuntament durant l'entrenament \citep{Min2018}:
        \begin{itemize}
            \item La \textbf{funció de cost auxiliar}, que depèn de la capacitat del codificador d'obtenir una \gls{rl} eficient que permeti que el descodificador sigui capaç de regenerar l'entrada. Garanteix obtenir \gls{lf} rellevants.
            \item La \textbf{funció de cost de \textit{clustering}}, que depèn de l'algoritme de \textit{clustering} i de que les \gls{lf} obtingudes siguin adequades per realitzar \textit{clustering}. Assegura obtenir una bona \gls{cq}.
        \end{itemize}

        L'aproximació més bàsica de l'arquitectura \gls{ae} es pot construir utilitzant \glspl{mlp} simètrics pel codificador i descodificador. Encara que és fàcil d'implementar, el model generat conté un gran nombre d'hiperparàmetres i esdevé difícil d'optimitzar i balancejar les dues funcions de cost \citep{Karim2021}. A més, no s'aplica cap restricció a la funció de representació i per tant no garanteix obtenir una bona \gls{cq}.

        A la literatura s'han descrit diferents aproximacions a la idea del \gls{ae}, que prenen com a base altres arquitectures. Algunes d'aquestes aproximacions permeten aconseguir un model generatiu, que és capaç de generar dades similars a les d'entrenament. Alguns d'aquestes aproximacions són:

        \textbf{\Gls{cae}:} l'arquitectura \gls{ae} estàndard no dona bons resultats per trobar patrons en imatges (dades amb invariança espacial). Per combatre-ho, es poden combinar amb \gls{cnn} utilitzant convolucions al codificador del \gls{ae} (i les corresponents desconvolucions al descodificador), on els filtres de la \gls{cnn} són un paràmetre que es pot optimitzar (en lloc de construir-los a mà com en una \gls{cnn} estàndard).

        \textbf{\Gls{vae}:} aquesta aproximació es basa en mètodes Bayesians, el que li atorga certa robustesa estadística que no tenen altres arquitectures. Es tracten les dades com a mostres d'una distribució desconeguda i s'aplica una restricció al codificador perquè les representi com una barreja de distribucions conegudes (per exemple, gaussianes). Els paràmetres que aprèn la xarxa són els paràmetres de les distribucions (mitjana i variància). El descodificador reconstrueix les mostres originals, el que permet generar mostres aleatòries semblants a la distribució original desconeguda. S'explica amb més detall a la \cref{s:metodes_vae}.

        \textbf{\Gls{lstmae}:} de manera similar al cas del \gls{cae}, l'arquitectura \gls{ae} estàndard no funciona bé amb dades seqüencials, però es pot combinar amb l'arquitectura \gls{lstm}. El codificador es forma combinant capes \gls{lstm}, on la última capa codifica un vector que representa l'entrada de la xarxa. El descodificador, també format per capes \gls{lstm}, pren aquest vector (replicat per adaptar-lo a l'entrada de la primera capa \gls{lstm}) i reconstrueix l'entrada original.

        \textbf{\Gls{aae}:} aquesta aproximació pren la idea del discriminador de la arquitectura \gls{gan} i la introdueix al \gls{ae}. L'objectiu és representar les dades originals com una barreja de distribucions conegudes, de manera similar al \gls{vae}. En aquest cas la xarxa està formada per tres parts: el codificador, que redueix l'entrada a un codi, el descodificador, que reconstrueix la entrada a partir del codi, i un discriminador. El discriminador intenta discriminar entre els codis generats i punts mostrejats aleatòriament de la distribució escollida. El resultat és que els codis generats acaben aproximant-se a aquesta distribució \citep{Makhzani2015}.

        \textbf{\Gls{dae}:} aquesta tècnica entrena el \gls{ae} introduint soroll a l'entrada del codificador i reconstruint les dades originals amb el descodificador. Això permet aprendre una representació més robusta de les dades i també la reconstrucció de les dades originals a partir d'unes dades parcialment corrompudes.

        \textbf{\Gls{sae}:} diversos \gls{ae} es poden apilar per aconseguir representacions més comprimides de les dades d'entrada. S'entrena un primer \gls{ae} per comprimir i reconstruir les dades d'entrada i es pren el codi comprimit. Aquest codi es pren com l'entrada d'un segon \gls{ae}, que s'entrena per comprimir i reconstruir el codi. Aquest procés es repeteix fins a aconseguir el nombre de capes desitjat. Finalment, s'ordenen les capes codificadores i descodificadores de cada \gls{ae} seqüencialment, formant un \gls{sae}, i s'entrena una última vegada el model sencer per ajustar tots els pesos. D'aquesta manera s'obté un model capaç de codificar les dades en un espai dimensional més reduït.

        En aquest \gls{tfm}, es desenvoluparà un model de \textit{deep clustering} utilitzant una arquitectura \gls{vae}, ja que les seves característiques el fan un model interessant. El model probabilístic li atorga certa robustesa estadística que pot ser útil al extreure conclusions del resultats obtinguts. Alhora, la capacitat generativa pot permetre generar mostres artificials semblants a les originals, el que podria servir com a tècnica d'augment de dades (\textit{data augmentation}).


\chapter{Materials i mètodes}
    %En aquests apartats, cal descriure:
    %\begin{itemize}
    %    \item Els aspectes més rellevant del disseny i desenvolupament del treball.
    %    \item La metodologia triada per a fer aquest desenvolupament, descrivint les alternatives possibles, les decisions preses, i els criteris utilitzats per prendre aquestes decisions.
    %    \item Els productes obtinguts.
    %\end{itemize}
    %\textbf{L’estructuració dels capítols pot variar segons el tipus de treball.}

    \section{Mètodes de \textit{clustering} clàssiques}
    \label{s:metodes_classic}

\todo{Breu explicació mètodes de clustering K-means, GMM, Agglomeratiu.}


    \section{Models de \textit{deep clustering}}
    \label{s:metodes_dc}

    En aquest \gls{tfm}
    es pretén implementar un model de \textit{deep clustering} en dades metabòliques.
    Com s'ha descrit a les \cref{s:state_deep,s:state_ae},
    existeix una gran varietat d'arquitectures de xarxes neuronals descrites a la literatura.
    Intentar estudiar totes les possibilitats seria inabastable donat el temps disponible,
    pel que ha sigut necessari acotar l'estudi a una única arquitectura.
    S'ha decidit implementar una aproximació basada en \gls{vae},
    donat que presenta algunes característiques interessants.

    Com s'ha mencionat a la \cref{s:state_ae},
    l'arquitectura \gls{vae} és una implementació específica de \gls{ae}
    basada en mètodes Bayesians.
    Això li atorga al model certa robustesa estadística
    que no trobem en altres arquitectures.
    A més,
    la \gls{rl} apresa permet la generació de mostres artificials
    similars a les dades d'entrenament.
    Això últim pot resultar útil com a tècnica d'augment de dades.

    \subsection{Deep embedded clustering}
    \label{s:metodes_dec}

\todo{Explicació DEC. Traspassar aquí l'explicació del mètode de clustering.}


    \subsection{Variational Autoencoder}
    \label{s:metodes_vae}

        A continuació es fa una breu presentació del model \gls{vae},
        basada en les referències \citep{Kingma2014,Kingma2019}.
        No s'entrarà en profunditat en les explicacions matemàtiques.
        Per una explicació més detallada es recomana llegir les fonts originals.

        En l'arquitectura \gls{ae},
        el codificador aprèn una representació de l'entrada $\mathbf{x}$
        en un espai dimensional reduït $\mathbf{z}$
        aplicant una transformació no lineal.
        Des de la perspectiva de \gls{fl},
        aquest espai reduït es correspon amb la \gls{rl}.
        El descodificador aprèn a reconstruir $\mathbf{x}$ a partir de $\mathbf{z}$
        aplicant una segona transformació no lineal.
        L'entrenament de les dues parts conjuntament
        aconsegueix que les \gls{lf} siguin una bona representació de les dades originals.

        En l'aproximació que pren el \gls{vae},
        s'assumeix que l'entrada $\mathbf{x}$ és un conjunt d'observacions de variables aleatòries,
        que provenen d'un procés desconegut
        amb una distribució $p^*(\mathbf{x})$.
        Aquesta distribució és desconeguda,
        de manera que intentem aproximar-la amb un model $p_\theta(\mathbf{x})$,
        on $\theta$ són els paràmetres del model.

        L'espai $\mathbf{z}$ es considera un conjunt de variables latents del model
        que segueixen una distribució $p_\theta(\mathbf{z})$.
        Les variables latents són variables que estan presents al model però no observem en les dades.
        Es pot fixar que $p_\theta(\mathbf{z})$ sigui una distribució fàcil de computar,
        per exemple una barreja de gaussianes.

        Sota aquesta premissa,
        el model a inferir és sobre una distribució conjunta $p_\theta(\mathbf{x,z})$
        de les variables observades i latents.
        L'objectiu del codificador és aprendre els paràmetres $\theta$
        de la distribució $p_\theta(\mathbf{z|x})$
        (la distribució posterior de $\mathbf{z}$ condicionada a $\mathbf{x}$)
        De la mateixa manera,
        l'objectiu del descodificador és aprendre els paràmetres $\theta$
        de la distribució $p_\theta(\mathbf{x|z})$.
        A la \cref{fig:vae_kingma} s'il·lustra
        com el model relaciona els espais $\mathbf{x}$ i $\mathbf{z}$.

        \begin{figure}
            \centering
            \includegraphics[width=0.5\textwidth]{kingma2019_vae.pdf}
            \caption{
                Esquema del model \gls{vae}.
                Les dades observades $\mathbf{D}$ (\textit{dataset})
                són una mostra d'unes variables de l'espai $\mathbf{x}$,
                que segueix una distribució intractable.
                L'espai latent $\mathbf{z}$ s'escull que segueixi
                una distribució $p_\phi(\mathbf{z})$ fàcil de computar.
                El codificador (\textit{encoder})
                troba la distribució condicional de $\mathbf{z}$ donada $\mathbf{x}$,
                aconseguint una representació de les mostres a l'espai latent.
                El decodificador (\textit{decoder})
                troba la distribució condicional de $\mathbf{x}$ donada $\mathbf{z}$,
                aconseguint la reconstrucció de les dades a partir de l'espai latent.
                Font: Kingma and Welling, 2019 \citep{Kingma2019}
            }
            \label{fig:vae_kingma}
        \end{figure}

    \subsection{Optimització dels paràmetres}
    \label{s:vae_optimitzacio}

        El codificador pretén aprendre els paràmetres $\theta$ òptims
        de la distribució $p_\theta(\mathbf{z|x})$,
        que està condicionada per la distribució marginal $p_\theta(\mathbf{x})$
        segons la equació:
        \begin{equation}
        \label{eq:cond_p_z_on_x}
            p_\theta(\mathbf{z|x}) =
            \frac {p_\theta(\mathbf{x,z})}
                  {p_\theta(\mathbf{x})}
        \end{equation}

        Aquesta distribució marginal en el model bé donada per:
        \begin{equation}
        \label{eq:marginal_p_x}
            p_\phi(\mathbf{x}) = \int p_\phi(\mathbf{x, z}) d\mathbf{z}
        \end{equation}

        La integral en la \cref{eq:marginal_p_x} provoca que
        la distribució marginal $p_\theta(\mathbf{x})$ no tingui un estimador eficient
        i sigui intractable computacionalment.
        Això provoca que no es pugui diferenciar respecte als paràmetres $\theta$
        i optimitzar-la pel mètode del descens de gradients.
        En conseqüència,
        tampoc es poden optimitzar els paràmetres de la distribució $p_\theta(\mathbf{z|x})$.

        En el seu lloc,
        es pot inferir un model $q_\phi(\mathbf{z|x})$
        que aproximi la distribució intractable $p_\theta(\mathbf{z|x})$,
        on els paràmetres $\phi$ s'aprenen a través de la optimització
        dels pesos i biaixos de la xarxa neuronal.

        La diferència entre aquestes dues distribucions
        es pot calcular mitjançant la divergència de \gls{kl}:
        $D_{KL} ( q_\phi(\mathbf{z|x}) \| p_\theta(\mathbf{z|x}) )$.
        L'objectiu és minimitzar aquesta divergència,
        però no és possible calcular-la directament
        donada la intractabilitat de $p_\theta(\mathbf{z|x})$.

        Aquesta divergència es relaciona amb la distribució marginal
        segons la següent igualtat:
        \begin{equation}
            \label{eq:kl}
            \log p_\theta(\mathbf{x}) =
            \mathcal{L}_{\theta,\phi}(\mathbf{x}) +
            D_{KL} (
            q_\phi(\mathbf{z|x}) \|
            p_\theta(\mathbf{z|x})
            )
        \end{equation}

        Per tant, es pot minimitzar la divergència maximitzant el terme $\mathcal{L}_{\theta,\phi}(\mathbf{x})$.
        Aquest terme s'anomena \gls{elbo}
        ja que la divergència \gls{kl} és sempre no negativa i
        per tant \gls{elbo} és el límit inferior de $\log p_\theta(\mathbf{x})$
        (evidència de $\mathbf{x}$):
        \begin{equation}
            \label{eq:lim_inf}
            \log p_\theta(\mathbf{x}) \ge
            \mathcal{L}_{\theta,\phi}(\mathbf{x})
        \end{equation}

        El terme \gls{elbo} es calcula segons la següent equació:
        \begin{equation}
        \label{eq:elbo}
            \mathcal{L}_{\theta,\phi}(\mathbf{x}) =
            \mathbb{E}_{q_\phi(\mathbf{z|x})}
                [ \log p_\theta(\mathbf{x|z}) ] -
            \mathbb{E}_{q_\phi(\mathbf{z|x})} \left[
                \log \frac {q_\phi(\mathbf{z|x})}
                           {p_\theta(\mathbf{z})}
            \right]
        \end{equation}

        La maximització del \gls{elbo} implica:
        \begin{itemize}
            \item Maximitzar el primer terme de la equació,
            la esperança de $\log p_\theta(\mathbf{x|z})$,
            que es correspon amb optimitzar els paràmetres del descodificador.
            És a dir, s'aconsegueix una millor reconstrucció.

            \item Minimitzar el segon terme,
            la esperança de $\log ( q_\phi(\mathbf{z|x}) / p_\theta(\mathbf{z}) )$,
            que és la divergència \gls{kl} entre
            la distribució inferida pel codificador $q_\phi(\mathbf{z|x})$
            i la distribució $p_\theta(\mathbf{z})$ fixada de l'espai latent.
            És a dir, s'aconsegueix millorar la representació obtinguda pel codificador.
        \end{itemize}

    \subsection{Descens de gradients}
    \label{s:vae_gradients}

        Per optimitzar els paràmetres de la xarxa neuronal
        de tal manera que es maximitzi el \gls{elbo},
        s'utilitza el descens de gradients.
        Per tant, cal calcular els gradients del \gls{elbo}
        amb respecte als paràmetres $\theta$ i $\phi$.

        El \gls{elbo} es pot reformular de la següent manera,
        el que facilitarà el càlcul dels gradients:
        \begin{equation}
        \label{eq:elbo_2}
            \mathcal{L}_{\theta,\phi}(\mathbf{x}) =
            \mathbb{E}_{q_\phi(\mathbf{z|x})} [
                \log p_\theta(\mathbf{x,z}) -
                \log q_\phi(\mathbf{z|x})
            ]
        \end{equation}

        I els gradients es calculen segons:
        \begin{gather}
        \label{eq:gradient_theta}
            \nabla_\theta \mathcal{L}_{\theta,\phi}(\mathbf{x}) =
            \nabla_\theta \mathbb{E}_{q_\phi(\mathbf{z|x})} [
            \log p_\theta(\mathbf{x,z}) -
            \log q_\phi(\mathbf{z|x})
            ]
        \\
        \label{eq:gradient_phi}
            \nabla_\phi \mathcal{L}_{\theta,\phi}(\mathbf{x}) =
            \nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z|x})} [
            \log p_\theta(\mathbf{x,z}) -
            \log q_\phi(\mathbf{z|x})
            ]
        \end{gather}

        Per la regla de la integral de Leibniz, el gradient en la \cref{eq:gradient_theta} es pot moure a dins de la esperança i això permet la seva computació. Però el mateix no és possible en la \cref{eq:gradient_phi}, ja que l'esperança està en funció de $\phi$.

        Per resoldre aquest problema es planteja la següent solució:
        reparametritzar la variable $\mathbf{z} \sim q_\theta(\mathbf{z|x})$
        com una funció d'una altra variable aleatòria $\epsilon$, donades $\mathbf{x}$ i $\phi$:
        \begin{gather}
        \label{eq:epsilon}
            \epsilon \sim p(\epsilon) \\
        \label{eq:z_reparam}
            \mathbf{z} = g(\phi, \mathbf{x}, \epsilon)
        \end{gather}

        on la funció $g(\cdot)$ és una transformació determinista.
        Aquesta reparametrització s'il·lustra a la \cref{fig:reparametritzacio}.
        Llavors, el \gls{elbo} es pot reescriure
        en funció de $\mathbb{E}_{p_(\epsilon)}$
        en lloc de $\mathbb{E}_{q_\phi(\mathbf{z|x})}$,
        de manera que ``s'externalitza'' la aleatorietat de la variable $\mathbf{z}$
        a la nova variable $\epsilon$.
        La nova forma del gradient del \gls{elbo} en respecte a $\phi$ és:
        \begin{equation}
        \label{eq:gradient_phi_reparam}
            \nabla_\phi \mathcal{L}_{\theta,\phi}(\mathbf{x}) =
            \nabla_\phi \mathbb{E}_{p(\epsilon)} [
                \log p_\theta(\mathbf{x,z}) -
                \log q_\phi(\mathbf{z|x})
            ]
        \end{equation}

        Després d'aquesta reparametrització,
        es poden calcular els gradients del \gls{elbo}
        respecte als paràmetres $\phi$ i $\theta$,
        i així optimitzar-los mitjançant el descens de gradients.

        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{kingma2019_reparam.pdf}
            \caption{
                Il·lustració de la reparametrització de $\mathbf{z}$.
                Per optimitzar els paràmetres $\theta$
                es necessita calcular els gradients de la funció d'optimització $f$
                amb respecte a $\theta$.
                En la figura de l'esquerra (forma original)
                no es pot diferenciar $f$ amb respecte a $\theta$
                perquè no es poden propagar endarrere els gradients
                a través de la variable aleatoria $\mathbf{z}$.
                Després de la reparametrització (figura de la dreta),
                la aleatorietat queda ``externalitzada'' a la variable $\epsilon$,
                i $\mathbf{z}$ depèn d'una funció $g(\cdot)$ determinista.
                Aquesta configuració sí permet propagar endarrere els gradients.
                Font: Kingma and Welling, 2019 \citep{Kingma2019}
            }
            \label{fig:reparametritzacio}
        \end{figure}

    \subsection{Implementació del model}
    \label{s:vae_implementacio}

        Un cop presentada la base teòrica,
        a continuació es resumeix com s'ha implementat el model \gls{vae}
        que s'ha construït en aquest \gls{tfm}.
        S'ha escollit que la variable latent $\mathbf{z}$
        segueixi una distribució barreja de gaussianes:

        $$\mathbf{z} \sim \mathcal{N}(\mu, \sigma^2)$$

        on $\mu$ és el vector de mitjanes de les distribucions
        i $\sigma^2$ és la matriu de variàncies.
        Per simplificar els càlculs s'ha escollit una matriu diagonal,
        de mode que $\sigma^2$ es pot escriure com un vector.

        Així, el codificador i el descodificador del \gls{vae} són
        dues xarxes neuronals amb paràmetres $\theta$ i $\phi$, respectivament.
        Aquests paràmetres són l'arquitectura de la xarxa neuronal,
        juntament amb els pesos pesos i biaixos de cada neurona.

        El codificador pren com entrada els valors de les variables observades,
        en forma d'un vector $\mathbf{x}$ per cada observació,
        i té com a sortida dos vectors:
        un vector $\mu$ amb les mitjanes de la barreja de gaussianes,
        i un vector $\sigma^2$ amb les corresponents variàncies
        (codificades com a $\log \sigma^2$ per motius de computació).

        El descodificador pren com entrada un punt de l'espai latent $\mathbf{z}$
        i té com a sortida un vector amb la mateixa forma que $\mathbf{x}$.

        En l'arquitectura \gls{ae},
        durant la fase d'entrenament
        es connecta la sortida del codificador a l'entrada del descodificador.
        Així, les dues xarxes s'entrenen conjuntament
        utilitzant el descens de gradient estocàstic.
        En el caso de l'arquitectura \gls{vae},
        és necessari aplicar la reparametrització
        explicada en la \cref{s:vae_gradients}.
        Un cop aplicada, es pot reescriure $\mathbf{z}$ com:

        $$\epsilon \sim \mathcal{N}(0, \mathbf{I})$$
        $$\mathbf{z} = \mu + \sigma \odot \epsilon$$

        on $\odot$ és el producte per elements.

        En la implementació del \gls{vae},
        s'insereix una capa neuronal sense pesos o biaixos
        enter el codificador i el descodificador.
        Aquesta capa pren com entrada la sortida del codificador
        (vectors $\mu$ i $\sigma^2$),
        obté una mostra aleatòria de la distribució de $\epsilon$
        i calcula un valor de $\mathbf{z}$,
        que passa a l'entrada del descodificador.
        A la \cref{fig:vae_kingma} es mostren esquemàticament
        les diferents parts del \gls{vae}.

        Per entrenar la xarxa neuronal en conjunt,
        s'han definit dues funcions de cost
        (il·lustrades a la \cref{fig:vae_losses})
        que corresponen als dos termes del \gls{elbo},
        segons la \cref{eq:elbo}:

        \begin{itemize}
            \item \textbf{Funció de cost de la reconstrucció:}
            computa la diferència entre l'entrada del codificador
            i la sortida del descodificador
            (la reconstrucció de l'entrada).
            Minimitzant-la, es maximitza el terme primer terme del \gls{elbo}:
            $\mathbb{E}_{q_\phi(\mathbf{z|x})} [ \log p_\theta(\mathbf{x|z}) ]$.

            \item \textbf{Funció de cost de regularització:}
            computa la distància \gls{kl}
            entre la distribució de $\mathbf{z} \sim \mathcal{N}(\mu, \sigma^2)$
            i una distribució $\mathcal{N}(0, \mathbf{I})$.
            Minimitzant-la, es minimitza el segon terme del \gls{elbo}:
            $\mathbb{E}_{q_\phi(\mathbf{z|x})} [ \log ( q_\phi(\mathbf{z|x}) / p_\theta(\mathbf{z}) ) ]$.
            S'anomena així ja que actua com a regularitzador
            de la sortida del codificador,
            assegurant que l'espai latent segueixi la distribució desitjada.
        \end{itemize}

        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{karim2021_vae.jpeg}
            \caption{
                Representació esquemàtica d'una arquitectura \gls{vae}.
                El codificador aprèn a representar l'entrada $\mathbf{x}$
                en un espai latent $\mathbf{z}$
                que segueix una distribució barreja de gaussianes,
                amb un vector de mitjanes $\mu$ i un vector de variancies $\sigma$.
                S'utilitza la reparametrització de $\mathbf{z}$ en funció de $\epsilon$.
                El descodificador reconstrueix l'entrada
                a partir de la representació a l'espai latent.
                Font: Karim \textit{et. al.}, 2021 \citep{Karim2021}
            }
            \label{fig:vae_min}
        \end{figure}

    \subsection{Varational Deep Embedding}
    \label{s:metodes_vade}

\todo{Explicació model VaDE}


    \subsection{Clustering}
    \label{s:clustering}

        Finalment, un cop s'ha entrenat el \gls{vae}
        s'ha aconseguit un model amb un codificador
        és capaç de representar les dades $\mathbf{x}$ en un espai latent $\mathbf{z}$,
        i un descodificador capaç de generar mostres simulades
        a partir de $\mathbf{z}$.
        És a dir, el model ha après unes \gls{lf} rellevants de les dades
        que permeten obtenir una \gls{rl} eficient.

        El següent pas és aplicar una tècnica de \textit{clustering}
        sobre les representacions de l'espai latent $\mathbf{z}$.
        Per aconseguir-ho,
        es construeix un model de \textit{clustering}
        basat en una nova capa neuronal.
        Els pesos de cada neurona d'aquesta capa
        corresponen a la posició del centroide d'un clúster.
        Una limitació d'aquesta tècnica és que requereix
        establir el nombre $k$ de clústers prèviament.

        La capa pren com entrada un punt de $\mathbf{z}$
        i calcula la distància del punt a cada centroide.
        La sortida de la capa és un vector de longitud $k$ que suma 1,
        on cada valor representa la probabilitat d'assignació del punt a cada clúster.
        Aquesta assignació s'interpreta com una distribució $Q$.

        A continuació es defineix una nova funció de cost,
        la \textbf{funció de cost de \textit{clustering}},
        com la distància \gls{kl} entre la distribució $Q$
        i una distribució auxiliar $P$.
        Escollint una distribució auxiliar adequada,
        al minimitzar aquesta distància
        s'aconsegueix millorar la \gls{cq}:
        es minimitza la distància de cada punt al seu centroide
        alhora que es maximitza la distància a la resta de centroides.

        La distribució auxiliar $P$ es calcula
        a partir de $Q$:
        \begin{equation}
            \label{eq:clustering_aux}
            p_{ij} =
            \frac {q_{ij}^2 / f_j}
                 {\sum_j' q_{ij}^2 / f_j'}
        \end{equation}

        on $p_{ij}$ i $q_{ij}$ són respectivament la probabilitat
        de $P$ i $Q$ per la observació $i$ i el clúster $j$,
        i $f_j$ és la freqüència del clúster $j$.

        Un cop construït el nou model,
        s'entrena conjuntament aplicant les tres funcions de cost
        (funció de cost reconstrucció, de regularització i de \textit{clustering}),
        ja que si s'optimitzés només respecte a la funció de cost de \textit{clustering}
        es corre el risc que es perdin les \gls{lf} apreses.

        Per establir uns pesos inicials,
        abans d'iniciar l'entrenament
        es passen les dades d'entrenament pel codificador
        per obtenir les seves representacions en l'espai $\mathbf{z}$.
        Seguidament s'aplica l'algoritme \textit{K-means}
        i es pren la posició dels centroides.

        Finalment, s'entrena la xarxa utilitzant el descens de gradients estocàstic.
        El model aprendrà una nova representació en l'espai latent $\mathbf{z}$,
        juntament amb una posició òptima dels centroides que defineixen cada clúster,
        que en conjunt optimitzin la \gls{cq}
        mantenint unes \gls{lf} rellevants a la \gls{rl}.

        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{min2018_losses.png}
            \caption{
                Esquema de les funcions de cost
                aplicades durant l'entrenament
                d'un model de \textit{deep clustering}
                basat en arquitectura \gls{vae}.
                Font: adaptada de Min \textit{et. al.}, 2018 \citep{Min2018}
            }
            \label{fig:vae_losses}
        \end{figure}


    \section{Mètriques d'avaluació}
    \label{s:metrics}

        Per validar la \gls{cq}
        s'han utilitzat dos tipus de mètriques:
        validació externa i validació interna.
        Les mètriques de validació externa
        es basen en comparar els clústers obtinguts
        amb una partició de les dades coneguda prèviament,
        que es considera la partició \textit{correcta}.
        Les mètriques de validació interna
        no utilitzen dades externes i
        es basen en mesures observables
        en la pròpia estructura dels clústers.

        Donat que la tècnica de \textit{clustering}
        és una tècnica d'aprenentatge no supervisada,
        normalment s'utilitzen exclusivament les mètriques de validació interna
        ja que no es coneix l'estructura real de les dades.
        En el cas d'aquest \gls{tfm},
        les dades utilitzades provenen d'un estudi clínic
        i estan etiquetades segons el diagnòstic de cada pacient,
        per tant s'han pogut utilitzar també mètriques de validació externa.

        Cap mètrica per si sola és eficaç
        per avaluar la \gls{cq} \cite{Palacio-Nino2019},
        de manera que s'han seleccionat quatre mètriques:
        una basada en validació interna
        i tres basades en validació externa.
        Es descriuen a continuació:

        \paragraph{Mètriques de validació interna}
            La \gls{cq} es pot mesurar en funció de
            la cohesió
            (la proximitat entre els elements dins d'un mateix clúster),
            i el nivell de separació entre els diferents clústers.

%            \newacronym{sse}{SSE}{suma de quadrats dels errors dintre de clústers
%                                  (\textit{sum of squared errors within clusters})}
%            \newacronym{ssb}{SSB}{suma de quadrats dels errors entre clústers
%                                  (\textit{sum of squared errors between clusters})}
%            \newacronym{ch}{CH}{coefficient de Calisnki-Harabasz}
%
%            La cohesió es pot mesurar
%            mitjançant la \gls{sse}:
%            \begin{equation}
%            \label{eq:sse}
%                SSE(C_i) = \sum_{x \in C_i} d(c_i, x)^2
%            \end{equation}
%            on $x$ és un punt dintre del clúster $C_i$
%            i $c_i$ és el centroide del clúster.
%
%            De manera similar,
%            el nivell de separació es pot mesurar
%            mitjançant la \gls{ssb}:
%            \begin{equation}
%            \label{eq:ssn}
%                SSB = \sum_{i=1}^{K} m_i d(c_i, c)^2
%            \end{equation}
%            on $m_i$ és el nombre d'observacions
%            en el clúster $i$,
%            $c_i$ el seu centroide
%            i $c$ la mitjana global.
%
%            Les dues mesures es poden combinar en una sola mètrica
%            utilitzant el \gls{ch}:
%            \begin{equation}
%            \label{eq:ch}
%                CH = \frac
%                { \frac{SSB_M}{M-1} }
%                { \frac{SSE_M}{M} }
%            \end{equation}
%            on $M$ és el nombre de clústers.


            Una de les mètriques més comuns
            per avaluar aquestes dues característiques
            és el coeficient silueta.
            Es computa de la següent manera,
            segons les equacions a continuació:
            per cada punt $i$ es computen
            la distància mitja $a(i)$ a tots els punts $j$ dins el mateix cluster $C_a$,
            la mínima distància mitja $b(i)$ a tots els punts $j$ dins cada cluster $C_b$ diferent de $C_a$
            i el seu coeficient silueta $s(i)$.
            Finalment, es computa la silueta global $S$
            prenent la mitjana de les siluetes de tots els punts.

            \begin{gather}
                a(i) = \frac{1}{|C_a|}
                \sum_{j \in C_a, i \neq j} d(i,j)
                \\
                b(i) = \min_{C_b \neq C_a}
                \frac{1}{|C_b|}
                \sum_{j \in C_b} d(i,j)
                \\
                s(i) = \frac
                {b(i) - a(i)}
                {\max \{a(i), b(i)\}}
                \\
                \label{eq:sil}
                S = \frac{1}{n}
                \sum_{i=1}^{n} s(i)
            \end{gather}

            El coeficients silueta pren un valor en l'interval $[-1, 1]$,
            on valors positius indiquen una bona separació entre clústers,
            valors negatius indiquen que els clústers estan barrejats entre ells,
            i un coeficient de zero indica que les dades estan distribuïdes uniformement.

        \paragraph{Mètriques de validació externa}
            Si es té una partició de referència $P$,
            la \gls{cq} es pot mesurar comparant-la
            amb els clústers $C$ resultants de l'algoritme de \textit{clustering}.
            Es construeix un taula de contingència
            i es comparen les parelles d'observacions
            trobades en el mateix o diferents clústers
            en les particions $P$ i $C$.
            S'extreuen els següents indicadors:
            \begin{description}
                \item[TP]
                nombre de parelles trobades al mateix clúster
                tant en $C$ com en $P$.

                \item[FP] nombre de parelles trobades al mateix clúster en $C$,
                però en diferents clústers en $P$.

                \item[TN] nombre de parelles trobades en diferents clústers
                tant en $C$ com en $P$.

                \item[FP] nombre de parelles trobades en diferents clústers en $C$,
                però en el mateix clúster en $P$.
            \end{description}

            Amb aquests indicadors
            es pot computar una gran varietat de mètriques,
            que es poden classificar diverses famílies \citep{Palacio-Nino2019}.
            \begin{itemize}
                \item La família de conjunts coincidents
                es basa en assignar una correspondència
                entre els clusters $C$ i les particions $P$,
                i mesurar la similaritat entre els conjunts.

                \item La família d'igual a igual parteix de l'assumpció
                que les observacions que es troben a la mateixa partició a $P$
                haurien d'esta també al mateix clúster $C$,
                i es basen en les correlacions entre parelles d'observacions.

                \item La tercera família es basa
                en conceptes de la teoria de la informació.
            \end{itemize}

\todo{Valor-F no utilitzat!!!. RI i MI s'utilitza els coeficients ajustats.}

            S'ha seleccionat una mètrica de cada família, respectivament:
            el valor-F,
            l'índex de Rand
            i el coeficient de informació mútua.

            \subparagraph{Valor-F}
                La precisió ($Pr$) mesura
                el nombre d'elements dins de cada clúster classificats correctament.
                La exhaustivitat ($R$ de \textit{recall} mesura
                el nombre d'elements dins de cada partició real classificats correctament.
                Aquests dos coeficients es poden combinar
                en una sola mètrica, l'anomenat valor-F:
                \begin{gather}
                \label{eq:precision}
                    Pr = \frac{TP}{TP + FP}
                \\
                \label{eq:recall}
                    R = \frac{TP}{TP + FN}
                \\
                \label{eq:f_measure}
                    F = \frac{2 Pr R}{Pr + R}
                \end{gather}

            \subparagraph{Índex de Rand}
                És l'equivalent a l'exactitud (\textit{accuracy})
                en un mètode d'aprenentatge supervisat.
                Calcula la similaritat entre les particions $C$ i $P$
                com la proporció d'el nombre d'elements classificats correctament
                respecte el nombre total d'elements:
                \begin{equation}
                \label{eq:rand}
                    RI = \frac{TP + TN}{TP + FP + TN + FN}
                \end{equation}

            \subparagraph{Coeficient de informació mútua}
                Mesura la reducció en la incertesa
                de l'assignació dels clústers $C$
                donada la partició coneguda $P$.
                \begin{equation}
                \label{eq:mi}
                    MI = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{p_i p_j}
                \end{equation}

                on $p_{ij}$ és la proporció d'observacions
                de la partició $i$ trobades al clúster $j$
                respecte el total d'observacions;
                $p_i$ i $p_j$ són les proporcions d'observacions
                trobades respectivament a la partició $i$
                i al clúster $j$.


    \section{Conjunts de dades}
    \label{s:dades}

    Per comparar els resultats dels diferents mètodes de \textit{clustering}
    s'han utilitzat tres conjunts de dades,
    dos d'accés públic i un privat.

    \paragraph{\acrshort{mnist}}
        El conjunt de dades \gls{mnist} \citep{Deng2012}
        és un conjunt de dades freqüentment utilitzat
        per l'entrenament de models de \textit{machine learning},
        especialment amb els relacionats amb el reconeixement o processat d'imatges.

        Es tracta d'un conjunt de 70.000 imatges
        de dígits escrits a mà,
        etiquetades en funció del dígit que representen.
        Cada imatge té una resolució de 28 x 28 píxels.
        En aquest \gls{tfm}, s'ha tractat cada imatge com una observació
        i els 784 píxels com 784 variables independents.

    \paragraph{Exposome Data Challenge Event}
        Es tracta de diversos conjunts de dades
        provinents d'un concurs anomenat \textit{Exposome Data Challenge Event} \citep{Maitre2022},
        que va tenir lloc a l'abril de 2021.
        En aquest concurs es van presentar una sèrie de conjunts de dades
        metabolòmiques, proteòmiques, de metilació del ADN i d'exposició ambiental
        observades en nens entre 6 i 11 anys,
        a més de dades fenotípiques
        i de covariables o possibles factors de confusió.

        L'objectiu del concurs era que els competidors
        presentessin formes innovadores d'analitzar les dades
        i trobar relacions rellevants en l'àmbit de la salut.

        S'han seleccionat dos conjunts de dades
        sobre els que comparar les diferents tècniques de \textit{clustering}:

        \begin{itemize}
            \item Dades metabolòmiques, formades per dos conjunts de dades,
            extretes de mostres de sèrum (177 metabolits) i orina (44 metabolits),
            formant un total de 221 variables.

            \item Dades d'exposició ambiental o exposoma,
            compostes per 222 indicadors d'exposició
            relacionats amb l'exposició a factors ambientals
            (contaminació de l'aire, soroll),
            l'exposició a productes químics
            (metalls, pesticides)
            i l'estil de vida dels individus
            (dieta, consum de tabac).
        \end{itemize}

        Per mesurar les mètriques de validació externa,
        s'han seleccionat algunes variables fenotípiques i de les covariables
        que s'han trobat que poden ser descriptives.

        En tots els casos,
        s'ha seleccionat només aquelles observacions
        que contenen dades a tots els conjunts de dades.
        En total, es disposa de 1152 observacions.

    \paragraph{Dades metabolòmiques privades}
        En tercer lloc, s'ha utilitzat un conjunt de dades metabolòmiques
        facilitat per un equip privat,
        provinents d'un estudi clínic
        on es pretén relacionar les expressions metabolòmiques
        amb diversos indicadors de salut.

        Es tracta d'un conjunt de 1120 observacions
        sobre 411 metabolits
        obtingudes en tres moments diferents sobre els mateixos pacients,
        acompanyades de 16 covariables
        de les cuals dues són categòriques
        (gènere, índex de risc d'enfermetats cardiovasculars)
        i la resta numèriques
        (edat i diversos indicadors de salut).

        L'entitat que ha proveït les dades
        ha anonimitzat la identitat dels pacients
        i no permet la publicació de les dades
        per protegir la seva confidencialitat.

\todo{Referenciar font dades privades.}



    \section{Metodologia}
    \label{s:metodologia}

    A continuació es descriu la metodologia que s'ha seguit
    per avaluar els diferents mètodes de \textit{clustering}
    utilitzant els tres conjunts de dades estudiats.

    Tot el codi utilitzat per implementar els models de \textit{deep clustering}
    i per dur a terme la metodologia que s'explica a continuació
    s'ha fet públic a un repositori a GitHub:
    \url{https://github.com/carlescn/MSc_bioinformatics_thesis}.

        \subsection{\acrshort{mnist}}
        \label{s:mnist}

        El conjunt de dades \gls{mnist}
        s'ha seleccionat per ser un exemple molt estudiat
        i sobre el que es coneix que
        es poden aconseguir bons resultats de \textit{clustering}
        amb diferents mètodes, clàssics i basats en \textit{deep learning}.

        S'ha utilitzat com a marc de referència
        per validar la funcionalitat
        dels diferents models de \textit{deep clustering} implementats,
        i seleccionar aquells que funcionin millor.

        \paragraph{Mètodes clàssics}
            Primerament,
            s'han aplicat tècniques de \textit{clustering} clàssiques
            (\gls{kmeans}, \gls{gmm})
            sobre les dades brutes
            i s'han mesurat les mètriques de \gls{cq}.

            Seguidament s'ha aplicat \gls{pca}
            i s'han seleccionat les primeres components principals
            que expliquen el 80\% de la variància.
            S'han repetit les tècniques clàssiques
            i s'han mesurat les mètriques.
            Aquestes mètriques s'han pres com a mesura de referència
            contra la que comparar el rendiment de les tècniques de \textit{deep clustering}.

        \paragraph{Mètodes de \textit{deep clustering}}
            A continuació,
            s'han aplicat diverses tècniques de \textit{deep clustering}
            en ordre de menor a major complexitat.
            En tots els casos s'ha utilitzat
            el mateix subcojunt de 60.000 imatges per entrenar el model
            i les 10.000 restants per avaluar el seu rendiment.
            Per adaptar les dades a un format que els models de \textit{deep learning} puguin tractar,
            s'han normalitzat aplicant la funció \textit{min-max}
            per acotar tots els valors dins d'un rang entre 0 i 1.

            Per tots els models s'ha utilitzat la mateixa configuració
            del \gls{ae} que forma la base del model:
            un codificador i descodificador simètrics
            basats en \gls{mlp},
            formats per tres capes ocultes amb 512, 512 i 2048 neurones respectivament.
            El número de neurones de la capa latent s'ha fixat al número de clústers.
            Totes les neurones tenen funció d'activació \gls{relu},
            excepte les capes d'entrada (sense funció d'activació)
            i sortida (codificador: sense activació, descodificador: funció d'activació sigmoide).

            Primer
            s'ha entrenat un \gls{ae} amb les dades d'entrenament i
            s'han codificat les dades de validació amb el model resultant.
            Seguidament s'ha aplicat \gls{kmeans} i \gls{gmm}
            sobre les dades transformades.
            S'ha repetit el mateix procés substituint el \gls{ae} per un \gls{vae}.

            En segon lloc
            s'ha entrenat i avaluat un model \gls{dec},
            inicialitzant els paràmetres del model
            primer amb els centroides trobats per \gls{kmeans}
            i després pels trobats amb \gls{gmm}.
            S'han mesurat les mètriques pels dos resultats.

            En tercer lloc
            s'ha entrenat i avaluat un model mixt VAE+DEC, que implementa
            un mòdul de \textit{clustering} de l'arquitectura \gls{dec},
            sobre la base d'un \gls{vae}.
            Els paràmetres del mòdul \gls{dec} s'han inicialitzat amb \gls{kmeans} i amb \gls{gmm}.

            Per últim
            s'ha entrenat i avaluat un model \gls{vade},
            inicialitzant els paràmetres amb \gls{gmm}.

        \paragraph{Avaluació dels mètodes}

            Finalment,
            s'han comparat totes les mètriques
            i s'han seleccionat aquells models de \textit{deep learning}
            que mostren un bon rendiment en relació amb la seva complexitat:
            \gls{dec} i \gls{vade}.

            En tots els casos,
            els models s'han avaluat fixant el nombre de clústers a 10,
            que és el nombre de classes reals de les dades.
            No s'ha evaluat els resultats amb diferents nombres de clústers.


        \subsection{Exposome Data Challenge Event}
        \label{s:exposome}

        Amb aquest conjunt de dades s'han comparat
        tres tècniques de \textit{clustering} clàssiques
        amb els dos models de \textit{deep clustering}
        seleccionats al pas anterior.
        Donat que en aquest cas sí es tracta de dades metabolòmiques,
        s'espera que el resultat sigui extrapolable
        a altres conjunts de dades del mateix tipus.

        S'han seleccionat 11 variables independents
        (veure \cref{t:exposome_vars}),
        que s'han utilitzat com a classes objectiu
        contra les que s'ha calculat les mètriques de validació externa.
        S'espera que la distribució dels clústers trobats
        s'aproximi a alguna d'aquestes variables.

        \begin{table}[h]
            \small
            \centering
            \begin{tabular}{ |c c c l| }
                \hline
                Conjunt de dades & Nom           & Núm. grups & Descripció \\
                \hline
                Fenotip          & birth\_weight & 4*         & Pes al néixer. \\
                Fenotip          & iq            & 4*         & Quocient d'intel·ligència. \\
                Fenotip          & behaviour     & 4*         & Comportament neurològic (índex). \\
                Fenotip          & asthma        & 2          & Incidència d'asma. \\
                Fenotip          & bmi           & 4*         & Índex de massa corporal. \\
                Covariables      & cohort        & 6          & Cohort d'inclusió a l'estudi. \\
                Covariables      & age           & 7          & Edat en anys. \\
                Covariables      & sex           & 2          & Gènere. \\
                Covariables      & education     & 3          & Nivell d'estudis de la mare. \\
                Covariables      & native        & 3          & Pares natius del país de naixement. \\
                Covariables      & parity        & 3          & Nombre d'embarassos previs (mare). \\
                \hline
            \end{tabular}
            \caption{Variables independents seleccionades del conjunt de dades Exposome Data Challenge Event.
                \newline
                * variables numèriques contínues. Convertides en 4 grups mitjançant quantils.}
            \label{t:exposome_vars}
        \end{table}

        En un primer pas
        s'ha buscat una configuració òptima del \gls{ae}
        que formarà la base dels models de \textit{deep clustering}.
        Donat que el conjunt de dades és relativament petit
        (1.152 observacions en 221 variables)
        s'han provat diverses configuracions
        fins aconseguir reduir al màxim el número de paràmetres entrenables de la xarxa,
        sense afectar negativament a la puntuació de la funció de cost.
        El model resultant conté tres capes internes amb 16, 16 i 128 neurones respectivament.
        Igual que en la secció anterior,
        totes les neurones tenen funció d'activació \gls{relu},
        excepte les capes d'entrada i sortida i les capes latents.

        A continuació, s'han avaluat diversos mètodes de \textit{clustering}
        clàssics i bastats en \textit{deep learning}.
        A la \cref{t:exposome_tecniques} es mostra un resum de les tècniques utilitzades.


        \paragraph{Dades metabolòmiques}
            S'han avaluat els diversos models de \textit{clustering}
            sobre el conjunts de dades metabolòmiques.
            Donat que les dades metabolòmiques en orina contenen un número reduït de variables,
            s'ha optat per unir-las a les dades metabolòmiques en sèrum
            obtenint així un únic conjunt de dades metabolòmiques.
            Previ al seu estudi, les dades s'han normalitzat amb la funció \textit{min-max}
            per acotar tots els valors dins d'un rang entre 0 i 1.

            Primer
            s'han aplicat tècniques de \textit{clustering} clàssiques
            (\gls{kmeans}, \gls{gmm}, \gls{aglo})
            sobre les dades brutes
            i sobre la transformació \gls{pca}
            (seleccionant les components principals que expliquen el 80\% de la variància).
            A continuació s'ha entrenat i avaluat
            els dos models de \textit{deep clustering} seleccionats
            (\gls{dec}, \gls{vade}).
            Motivat pels resultats poc prometedors obtinguts,
            s'han provat diverses aproximacions addicionals.

            En primer lloc s'han seleccionat les variables amb major variància,
            calculant la variància per cada un dels metabòlits en sèrum
            i seleccionant aquells que presenten una variància superior a la mitjana de les variàncies.
            El mateix procés s'ha repetit pels metabòlits en orina,
            i finalment s'han tornat a unir els dos conjunts en un de sol
            A continuació s'han avaluat els mateixos mètodes clàssics
            i basats en \textit{deep learning}.

            Donat que els resultats inicials no han semblat molt prometedors,
            s'ha intentat millorar la relació entre la mida de les dades
            i el nombre de paràmetres de dues maneres:

            \begin{itemize}
                \item Augmentar artificialment el nombre de dades.
                S'han generat mostres aleatòries d'una distribució normal
                amb mitjana 0 i desviació estàndard 0.01
                (un ordre de magnitud inferior inferior a la mitjana de les dades).

                A continuació s'han sumat aquestes mostres als valors de les dades originals.
                S'ha repetit el procés 10 vegades per obtenir un conjunt de dades
                amb 11.520 observacions,
                que presenten una distribució similar a les dades originals.

                Amb aquestes dades, s'han entrenat i avaluat els dos models de \textit{deep clustering.}

                \item Reduir el nombre de paràmetres entrenables de la xarxa neuronal.
                S'han implementat nous models \gls{dec} i \gls{vade},
                substituint les capes internes del tipus \gls{mlp}
                per capes convolucionals.
                Això permet reduir dràsticament el nombre de paràmetres del model,
                però assumeix que les dades presenten algun tipus d'estructura invariable.

                Primerament s'han entrenat i avaluat els dos models
                amb capes convolucionals d'una dimensió.
                A continuació s'ha fet el mateix amb capes convolucionals de dues dimensions,
                pel que ha sigut necessari dotar a les dades d'una estructura bidimensional artificial:
                s'han convertit els vectors de 221 valors de cada observació
                en matrius bidimensional de mida 16 x 16,
                afegint zeros a les posicions de la matriu que han quedat buides.

                Per cada una de les dues configuracions (1D, 2D),
                s'han implementat diversos models \gls{ae}
                amb diferents combinacions de mida del kernel,
                número de capes i
                número de neurones per capa,
                i s'ha seleccionat la combinació que aconsegueix
                una bona puntuació per la funció de cost
                amb el menor número de paràmetres:
                \begin{itemize}
                    \item 1D: capes internes: 1 (4 neurones), mida del kernel: 3.
                    \item 2D: capes internes: 2 (4 i 4 neurones), mida del kernel: 3.
                \end{itemize}

                Finalment, s'han entrenat i avaluat els models \gls{dec} i \gls{vade}
                per cada una de les dues configuracions.
            \end{itemize}

        \paragraph{Dades de l'exposoma}
            A continuació
            s'han avaluat les tècniques clàssiques (\gls{kmeans}, \gls{gmm}, \gls{aglo})
            i els models de \textit{deep learning} \gls{dec} i \gls{vade}
            (basats en \gls{mlp}).
            En aquestes dades no s'ha aplicat tècniques d'augment de dades
            ni models amb capes convolucionals.

            Els resultats obtinguts
            suggereixen la presència d'un fort efecte de lot,
            ja que totes les tècniques aconsegueixen un solapament gairebé perfecte
            amb els grups de la variable \textit{cohort}.

            Finalment,
            s'ha decidit aplicar una correcció per aquest efecte lot als dos conjunts de dades,
            separant les observacions pels grups de la variable \textit{cohort}
            restant la mitjana de cada grup i dividint per la seva desviació típica.
            Finalment, s'han tornat a unir les dades en l'ordre original
            i s'han normalitzat amb la funció \textit{min-max},
            abans de tornar a avaluar els mateixos cinc mètodes de clustering
            (\gls{kmeans}, \gls{gmm}, \gls{aglo}, \gls{dec} i \gls{vade}).

        \paragraph{Avaluació dels models}

            Per cada base de dades i mètode (amb les seves variacions),
            s'ha avaluat els resultats per un rang de clústers 2, 3, 4, 6 i 7,
            que coincideixen amb les classes de les variables categòriques seleccionades.

            A més de compilar les mètriques de tots els mètodes avaluats,
            s'ha estudiat de manera gràfica
            una possible interpretació dels clústers resultants dels diferents models.

            Per una banda,
            s'ha dibuixat, per cada mètode i número de clústers,
            una gràfica radial que representa la distribució multivariant
            dels conjunts de dades fenotípiques i covariables
            per cada un dels clústers.

            Per una altra banda,
            s'ha seleccionat les 20 variables amb més variabilitat de cada conjunt de dades
            (metabolòmiques, exposoma)
            i s'ha representat la seva distribució multivariant
            per cada un dels clústers de la mateixa manera.

            Finalment,
            S'han comparat entre elles les assignacions de clústers de cada un dels mètodes
            mitjançant un gràfic tipus \textit{heatmap},
            on cada fila representa una de les tècniques avaluades,
            les columnes representen les observacions del dataset,
            i el color representa l'assignació de cada mostra a un dels clústers.

\todo{Incloure referències als gràfics en els resultats o annexos.}


            \begin{table}[]
                \small
                \centering
                \begin{tabular}{|cccc|}
                    \hline
                    Conjunt de dades &
                    \multicolumn{2}{c}{Mètode} &
                    Variacions \\ \hline
                    \multirow{2}{*}{Metaboloma} &
                    Clàssic &
                    \begin{tabular}[c]{@{}c@{}}K-means\\ GMM\\ Aglo.\end{tabular} &
                    \begin{tabular}[c]{@{}c@{}}PCA\\ Selecció de variables\\ Correcció efecte lot\end{tabular} \\
                    &
                    Deep learning &
                    \begin{tabular}[c]{@{}c@{}}DEC\\ VaDE\end{tabular} &
                    \begin{tabular}[c]{@{}c@{}}Selecció de variables,\\ Augment de dades,\\ MPL / Conv.\\ Correcció efecte lot\end{tabular} \\ \hline
                    \multirow{2}{*}{Exposoma} &
                    Clàssic &
                    \begin{tabular}[c]{@{}c@{}}K-means\\ GMM\\ Aglo.\end{tabular} &
                    Correcció efecte lot \\
                    &
                    Deep learning &
                    \begin{tabular}[c]{@{}c@{}}DEC\\ VaDE\end{tabular} &
                    \begin{tabular}[c]{@{}c@{}}MPL\\ Correcció efecte lot\end{tabular} \\ \hline
                \end{tabular}
                \caption{Resum de les tècniques de \textit{clustering} utilitzades
                    en el conjunt de dades \textit{Exposome Data Challenge Event}}
                \label{t:exposome_tecniques}
            \end{table}

        \subsection{Dades metabolòmiques privades}
        \label{s:dades_privades}

        Aquest és l'últim conjunt de dades que s'ha estudiat.
        S'ha seguit la mateixa metodologia bàsica que en el cas anterior.

        Inicialment s'han provat diverses configuracions
        del \gls{ae} base i s'ha escollit un model amb dues capes internes tipus \gls{mlp}
        amb 16 i 32 neurones respectivament.
        Igual que en els casos anteriors,
        totes les neurones tenen funció d'activació \gls{relu}
        excepte les capes d'entrada, sortida i latents.

        Com a grups objectiu contra els que avaluar les mètriques,
        s'han seleccionat dues variables categòriques de les covariables
        que s'ha trobat mostren una distribució diferencial per la resta de covariables
        (indicadors biològics relacionats amb la salut):
        \textit{gender}, que té dos classes
        i codifica el gènere de l'individu,
        i \textit{CVrisk3}, que té tres classes
        i codifica un índex de risk de patir malalties cardiovasculars.

        Igual que en el cas anterior,
        les dades metabolòmiques s'han normalitzat amb la funció \textit{min-max}
        per acotar tots els valors dins d'un rang entre 0 i 1.

        A continuació s'han avaluat els mètodes de \textit{clustering} clàssics
        (\gls{kmeans}, \gls{gmm}, \gls{aglo})
        sobre les dades originals
        i la transformació \gls{pca},
        i s'han entrenat i avaluat els dos mètodes de \textit{deep clustering}
        (\gls{dec}, \gls{vade}).
        Tots els mètodes s'han avaluat fixant el número de clústers a 2 i 3.

        Finalment,
        s'ha compilat les mètriques obtingudes pels diferents mètodes
        i s'ha estudiat de manera gràfica
        una possible interpretació dels clústers
        utilitzant gràfiques radials.

        S'ha representat la distribució multivariant de totes les covariants
        per cada un dels mètodes i pels dos números de clústers.
        S'han seleccionat els 20 metabolits que presenten més variabilitat
        i s'ha representat també la seva distribució multivariant.

        Finalment,
        S'han comparat les assignacions de clústers de cada un dels mètodes
        mitjançant un gràfic tipus \textit{heatmap},
        on cada fila representa una de les tècniques avaluades,
        les columnes representen les observacions del dataset,
        i el color representa l'assignació de cada mostra a un dels clústers.


    \section{Eines informàtiques}
    \label{s:software}

        \paragraph{Software}
            Els models de \textit{deep clustering}
            s'han implementat utilitzant el software Keras \citep{Chollet2015},
            una eina basada en TensorFlow \citep{Ghemawat2016}
            que es va desenvolupar per l'àmbit de la recerca.
            Permet definir i entrenar models de xarxes neuronals profunds
            de manera relativament senzilla,
            però alhora possibilita interactuar a més baix nivell amb TensorFlow
            per realitzar operacions més complexes \citep{Ketkar2021}.

            Pràcticament la totalitat de l'estudi
            s'ha realitzat mitjançant el llenguatge de programació Python \citep{VanRossum2009},
            documentat en llibretes Jupyter Notebook \citep{Kluyver2016}.
            Com s'ha mencionat al inici de la secció,
            tot el codi està disponible a un repositori a GitHub.

        \paragraph{Hardware i serveis de computació \textit{on-line}.}

            L'equip informàtic del que disposa l'estudiant,
            si bé té capacitat per executar les tècniques de \textit{clustering} clàssiques,
            presenta limitacions a l'hora d'entrenar models
            basats en xarxes neuronals profundes,
            que consumeixen una gran quantitat de memòria RAM
            i requereixen de GPUs potents per una execució més o menys àgil.

            Per aquest motiu,
            ha sigut imprescindible utilitzar un servei de computació \textit{on-line}.
            Concretament, s'ha utilitzat el servei de pagament Paperspace Gradient\footnote{\url{https://docs.paperspace.com/gradient/}}.
            A la \cref{s:economica} es desglossa el cost d'aquest servei.



\chapter{Resultats}
    %Detalleu en aquest apartat els resultats obtinguts utilitzant la metodologia descrita a l’apartat anterior.
    %Recull dels resultats del treball. Hauria d'haver-hi una correspondència amb la metodologia en el sentit que els resultats és el que s'obté després d'haver aplicat la metodologia.
    %Les figures han d'estar explicades i citades en el text, com la \ref{fig:my_label}, en la qual es mostra l'error en funció de la distància, en unitats arbitràries. A totes les gràfiques ha d'haver el títol dels eixos.
    %\begin{figure}[!htbp]
    %    \centering
    %    \includegraphics[width=7truecm]{Rplotmanh.png}
    %    \caption{Error en funcio de la distància en unitats arbitràries.}
    %    \label{fig:my_label}
    %\end{figure}

\chapter{Discussió}
    %Discussió dels resultats en el context del projecte. És en aquest apartat on cobren sentit i en el qual es responen les preguntes de recerca i es mostra com els resultats donen resposta als problemes plantejats.
    %Aquesta part pot ser que no apliqui segons el tipus de treball.

\chapter{Valoració econòmica}
\label{s:economica}
    %En cas que correspongui, s'inclourà un apartat de ``Valoració econòmica del treball". Aquest apartat indicarà les despeses associades al desenvolupament i manteniment del treball, així com els beneficis econòmics obtinguts. Cal fer una anàlisi final sobre la viabilitat del producte.

    La realització d'aquest \gls{tfm} ha requerit d'una única despesa econòmica:
    una subscripció Paperspace Gradient,
    el servei de computació \textit{on-line}
    que es menciona a la \cref{s:software}.

    Encara que aquesta plataforma ofereix un servei gratuït,
    s'ha considerat insuficient
    ja que no garanteix la disponibilitat de màquines virtuals en qualsevol moment.

    S'ha subscrit el pla de pagament més econòmic,
    que ha cobert els requisits de poder de computació
    i disponibilitat.
    La subscripció ha tingut un cost de \$8 al mes
    i s'ha activat durant els mesos de novembre, desembre i generdurant,
    sumant per tant un total de \$24
    (equivalent a aproximadament 22 € en el moment de redactar aquest informe).

    Aquest servei només ha sigut necessari
    ja que no es disposa d'una màquina prou potent
    per entrenar els models neuronals que s'han implementat.

    Donat que el cost mensual d'aquest servei és petit,
    que no és estrictament necessari
    si es disposa d'una màquina suficientment potent,
    i que l'objectiu no ha sigut desenvolupar un producte econòmicament sostenible,
    si no implementar i avaluar una sèrie de models prototip,
    es considera que el cost ha sigut perfectament assumible.


\chapter{Conclusions i treballs futurs}

    \section{Conclusions}
    \label{s:conclusions}
        %Aquest capítol ha d'incloure:
        %\begin{itemize}
        %    \item Una descripció de les conclusions del treball:
        %    \begin{itemize}
        %        \item Un cop s’han obtingut els resultats quines conclusions s’extreu?
        %        \item Aquests resultats són els esperats? O han estat sorprenents? Per què?
        %    \end{itemize}
        %    \item Una reflexió crítica sobre l’assoliment dels objectius plantejats inicialment:
        %    \begin{itemize}
        %        \item Hem assolit tots els objectius? Si la resposta és negativa, per quin motiu?
        %    \end{itemize}
        %\end{itemize}


    \section{Línies de futur}
    \label{s:futur}
        %Les línies de treball futur que no s'han pogut explorar en aquest treball i han quedat pendents.

    \section{Seguiment de la planificació}
    \label{s:planificacio}
        %\begin{itemize}
        %    \item Una anàlisi crítica del seguiment de la planificació i metodologia al llarg del producte:
        %    \begin{itemize}
        %        \item S’ha seguit la planificació?
        %        \item La metodología prevista ha estat prou adequada?
        %        \item Ha calgut introduir canvis per garantir l’èxit del treball? Per què?
        %    \end{itemize}
        %    \item Dels impactes previstos a \ref{s:etic}, ètic-socials, de sostenibilitat i de diversitat, avaluaeu/esmenteu si s'han mitigat (si eren negatius) o si s'han aconseguit (si eren positius).
        %    \item Si han aparegut impactes no previstos a \ref{s:etic}, avaluar/esmentar com s'han mitigat (si eren negatius) o què han aportat (si eren positius).
        %\end{itemize}

\chapter{Glossari i abreviacions}
    %Definició dels termes i acrònims més rellevants utilitzats dins la Memòria.

%    \section{Glossari}

    %Abreviacions:
    \printglossary[type=\acronymtype, title={Abreviacions}]

%\chapter{Bibliografia} %\bibliography{} ja afegeix el \chapter{} corresponent!
\bibliographystyle{ieeetr}
\bibliography{TFM, TFM-software}


\newpage
\appendix
%Llistat d’apartats que són massa extensos per incloure dins la memòria i tenen un caràcter autocontingut (per exemple, manuals d’usuari, manuals d’instal·lació, etc.)
%Depenent del tipus de treball, és possible que no calgui afegir cap annex.

%\chapter{Exemple d'annex}
%    \section{Annex 1}
%
%
%\chapter{Exemple d'annex}
%    \section{Annex B}


\end{document}
