@article{Petersen2022,
abstract = {The Diet, Cancer and Health—Next Generations (DCH-NG) study is a large population-based cohort study that was established as a resource for transgenerational research. The cohort is an extension of the Diet, Cancer and Health (DCH) cohort. The aim of this paper was to describe the study design and methods and to investigate the representativeness of participants by comparing participants with non-participants with emphasis on socioeconomic determinants. In 2015–2019, children (G1), their spouses (G1P) and grandchildren (G2) of DCH cohort members were invited to participate. Participants completed questionnaires, a physical examination and collection of biological material. Information on general and sociodemographic variables was obtained by linkage to administrative registries in Denmark. The cohort includes 39,554 adult participants with complete data collection. Participants are represented in different family structures including 2- and 3-generation relationships, offspring-parents trios and siblings. The odds ratio for participation was highest among G1, females, middle-aged and married individuals and individuals with the highest education, highest income, occupations requiring high-level skills and residency near a study centre. The different family structures allow a range of studies with cohort and transgenerational designs. The pattern of more likelihood of participation in higher socioeconomic groups was similar to the pattern of participation in the DCH cohort and the general patterns in population-based studies. Accordingly, the study population has some limitations as to being representative of the general population. Yet, the DCH-NG cohort will provide valuable insight on the association between risk factor-disease relationships and the role of heredity on these associations.},
author = {Petersen, Kristina E.N. and Halkj{\ae}r, Jytte and Loft, Steffen and Tj{\o}nneland, Anne and Olsen, Anja},
doi = {10.1007/s10654-021-00832-7},
issn = {15737284},
journal = {European Journal of Epidemiology},
keywords = {Cancer and Health—Next Generations cohort,Cohort profile,Diet,Family study,Lifestyle diseases,Representativeness,Transgenerational transmission},
month = {jan},
number = {1},
pages = {117--127},
pmid = {34982312},
publisher = {Eur J Epidemiol},
title = {{Cohort profile and representativeness of participants in the Diet, Cancer and Health—Next Generations cohort study}},
url = {https://pubmed.ncbi.nlm.nih.gov/34982312/},
volume = {37},
year = {2022}
}
@article{Maitre2022,
abstract = {The exposome recognizes that individuals are exposed simultaneously to a multitude of different environmental factors and takes a holistic approach to the discovery of etiological factors for disease. However, challenges arise when trying to quantify the health effects of complex exposure mixtures. Analytical challenges include dealing with high dimensionality, studying the combined effects of these exposures and their interactions, integrating causal pathways, and integrating high-throughput omics layers. To tackle these challenges, the Barcelona Institute for Global Health (ISGlobal) held a data challenge event open to researchers from all over the world and from all expertises. Analysts had a chance to compete and apply state-of-the-art methods on a common partially simulated exposome dataset (based on real case data from the HELIX project) with multiple correlated exposure variables (P > 100 exposure variables) arising from general and personal environments at different time points, biological molecular data (multi-omics: DNA methylation, gene expression, proteins, metabolomics) and multiple clinical phenotypes in 1301 mother–child pairs. Most of the methods presented included feature selection or feature reduction to deal with the high dimensionality of the exposome dataset. Several approaches explicitly searched for combined effects of exposures and/or their interactions using linear index models or response surface methods, including Bayesian methods. Other methods dealt with the multi-omics dataset in mediation analyses using multiple-step approaches. Here we discuss features of the statistical models used and provide the data and codes used, so that analysts have examples of implementation and can learn how to use these methods. Overall, the exposome data challenge presented a unique opportunity for researchers from different disciplines to create and share state-of-the-art analytical methods, setting a new standard for open science in the exposome and environmental health field.},
archivePrefix = {arXiv},
arxivId = {2202.01680},
author = {Maitre, L{\'{e}}a and Guimbaud, Jean Baptiste and Warembourg, Charline and G{\"{u}}il-Oumrait, Nuria and Petrone, Paula Marcela and Chadeau-Hyam, Marc and Vrijheid, Martine and Basaga{\~{n}}a, Xavier and Gonzalez, Juan R. and Alfano, Rossella and Basu, Sanjib and Benavides, Jaime and Bros{\'{e}}us, Lucile and Brunius, Carl and Caceres, Alejandro and Carli, Matthew and Cazabet, R{\'{e}}my and Chattopadhyay, Shounak and Chen, Yun Hua and Chillrud, Lawrence and Conti, David and Gennings, Chris and Gouripeddi, Ramkiran and Iyer, S. Hari and Jedynak, Paulina and Li, Huichu and McGee, Glen and Midya, Vishal and Mistry, Sejal and Moccia, Chiara and Mork, S. Daniel and Pearce, L. John and Peruzzi, Michele and Pescador, Jimenez Marcia and Reimann, Brigitte and Roscoe, J. Charlotte and Shen, Xiaotao and Stratakis, Nikos and Wang, Ziyue and Wang, Congrong and Wheeler, David and Wilson, Ander and Wu, Qiong and Yu, Miao and Zhao, Yinqi and Zou, Fei and Zugna, Daniela and Chen, Ruizhe and Chung, Yu Che and Jang, Jiyeong and Turyk, Mary},
doi = {10.1016/j.envint.2022.107422},
eprint = {2202.01680},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maitre et al. - 2022 - State-of-the-art methods for exposure-health studies Results from the exposome data challenge event.pdf:pdf},
issn = {18736750},
journal = {Environment International},
keywords = {Environmental exposures,Exposome,Multi-omics,Multiple exposures,Statistical models},
month = {oct},
pages = {107422},
pmid = {36058017},
publisher = {Pergamon},
title = {{State-of-the-art methods for exposure-health studies: Results from the exposome data challenge event}},
volume = {168},
year = {2022}
}
@misc{Dahal2022,
author = {Dahal},
title = {{DeepNotes | Deep Learning Demystified}},
url = {https://deepnotes.io/deep-clustering#autoencoders-based https://deepnotes.io/sgd-momentum-adaptive},
urldate = {2022-11-10},
year = {2022}
}
@article{Dahal2019,
abstract = {Clustering is one of the most fundamental unsupervised tasks in machine learning and is elementary in the exploration of high volume data. Recent works propose using deep neural networks for clustering, owing to their ability to learn powerful representations of the data. In this work, we present a novel clustering approach using deep neural networks that simultaneously learns feature representations and embeddings suitable for clustering by encouraging separation of natural clusters in the embedding space. More specifically, an autoencoder is employed to learn representations of the data. Then a mapping from autoencoder representation space to an embedding space is learned using a deep neural network we call Representation Network. This neural network promotes separation between natural clusters by minimizing cross-entropy between two probability distributions that denote pairwise similarity in autoencoder latent space and representation network's embedding space. The resultant optimization problem can be solved effectively by jointly training the autoencoder and the representation network using minibatch stochastic gradient descent and backpropagation. Ultimately we obtain a K-Means friendly embedding space. Experimental results show that despite being a simple model, the proposed approach outperforms a broad range of recent approaches on Reuters dataset, other autoencoder based models on MNIST dataset and produces consistently good results that are very competitive with other complex and hybrid models.},
author = {Dahal, Paras},
doi = {10.1109/BIGDATA.2018.8622629},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dahal - 2019 - Learning Embedding Space for Clustering from Deep Representations.pdf:pdf},
isbn = {9781538650356},
journal = {Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018},
keywords = {Clustering,deep learning,representation learning},
month = {jan},
pages = {3747--3755},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Learning Embedding Space for Clustering from Deep Representations}},
year = {2019}
}
@book{Ketkar2021,
abstract = {{\textcopyright} Springer-Verlag London 2014. All rights are reserved. Providing a broad but in-depth introduction to neural network and machine learning in a statistical framework, this book provides a single, comprehensive resource for study and further research. All the major popular neural network models and statistical learning approaches are covered with examples and exercises in every chapter to develop a practical working understanding of the content. Each of the twenty-five chapters includes state-of-the-art descriptions and important research results on the respective topics. The broad coverage includes the multilayer perceptron, the Hopfield network, associative memory models, clustering models and algorithms, the radial basis function network, recurrent neural networks, principal component analysis, nonnegative matrix factorization, independent component analysis, discriminant analysis, support vector machines, kernel methods, reinforcement learning, probabilistic and Bayesian networks, data fusion and ensemble learning, fuzzy sets and logic, neurofuzzy models, hardware implementations, and some machine learning topics. Applications to biometric/bioinformatics and data mining are also included. Focusing on the prominent accomplishments and their practical aspects, academic and technical staff, graduate students and researchers will find that this provides a solid foundation and encompassing reference for the fields of neural networks, pattern recognition, signal processing, machine learning, computational intelligence, and data mining.},
author = {Ketkar, Nikhil and Moolayil, Jojo},
booktitle = {Deep Learning with Python},
doi = {10.1007/978-1-4842-5364-9},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ketkar, Moolayil - 2021 - Deep Learning with Python.pdf:pdf},
isbn = {9781617296864},
title = {{Deep Learning with Python}},
year = {2021}
}
@inproceedings{Xie2015,
abstract = {Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1511.06335},
author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
booktitle = {33rd International Conference on Machine Learning, ICML 2016},
doi = {10.48550/arxiv.1511.06335},
eprint = {1511.06335},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie, Girshick, Farhadi - 2015 - Unsupervised Deep Embedding for Clustering Analysis.pdf:pdf},
isbn = {9781510829008},
month = {nov},
pages = {740--749},
publisher = {International Machine Learning Society (IMLS)},
title = {{Unsupervised deep embedding for clustering analysis}},
url = {https://arxiv.org/abs/1511.06335v2},
volume = {1},
year = {2016}
}
@article{Min2018,
abstract = {Clustering is a fundamental problem in many data-driven application domains, and clustering performance highly depends on the quality of data representation. Hence, linear or non-linear feature transformations have been extensively used to learn a better data representation for clustering. In recent years, a lot of works focused on using deep neural networks to learn a clustering-friendly representation, resulting in a significant increase of clustering performance. In this paper, we give a systematic survey of clustering with deep learning in views of architecture. Specifically, we first introduce the preliminary knowledge for better understanding of this field. Then, a taxonomy of clustering with deep learning is proposed and some representative methods are introduced. Finally, we propose some interesting future opportunities of clustering with deep learning and give some conclusion remarks.},
author = {Min, Erxue and Guo, Xifeng and Liu, Qiang and Zhang, Gen and Cui, Jianjing and Long, Jun},
doi = {10.1109/ACCESS.2018.2855437},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Min et al. - 2018 - A Survey of Clustering with Deep Learning From the Perspective of Network Architecture.pdf:pdf},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Clustering,data representation,deep learning,network architecture},
pages = {39501--39514},
publisher = {IEEE},
title = {{A Survey of Clustering With Deep Learning: From the Perspective of Network Architecture}},
url = {https://ieeexplore.ieee.org/document/8412085/},
volume = {6},
year = {2018}
}
@article{Palacio-Nino2019,
abstract = {Determining the quality of the results obtained by clustering techniques is a key issue in unsupervised machine learning. Many authors have discussed the desirable features of good clustering algorithms. However, Jon Kleinberg established an impossibility theorem for clustering. As a consequence, a wealth of studies have proposed techniques to evaluate the quality of clustering results depending on the characteristics of the clustering problem and the algorithmic technique employed to cluster data.},
archivePrefix = {arXiv},
arxivId = {1905.05667},
author = {Palacio-Ni{\~{n}}o, Julio-Omar and Berzal, Fernando},
eprint = {1905.05667},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palacio-Ni{\~{n}}o, Berzal - 2019 - Evaluation Metrics for Unsupervised Learning Algorithms.pdf:pdf},
journal = {Preprint at arXiv:1905.05667},
title = {{Evaluation Metrics for Unsupervised Learning Algorithms}},
url = {http://arxiv.org/abs/1905.05667},
year = {2019}
}
@article{Jiang2020,
abstract = {Clustering is among the most fundamental tasks in computer vision and machine learning. In this paper, we propose Variational Deep Embedding (VaDE), a novel unsupervised generative clustering approach within the framework of Variational Auto-Encoder (VAE). Specifically, VaDE models the data generative procedure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then the DNN decodes the latent embedding into observables. Inference in VaDE is done in a variational way: a different DNN is used to encode observables to latent embeddings, so that the evidence lower bound (ELBO) can be optimized using Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameterization trick. Quantitative comparisons with strong baselines are included in this paper, and experimental results show that VaDE significantly outperforms the state-of-the-art clustering methods on 4 benchmarks from various modalities. Moreover, by VaDE's generative nature, we show its capability of generating highly realistic samples for any specified cluster, without using supervised information during training. Lastly, VaDE is a flexible and extensible framework for unsupervised generative clustering, more general mixture models than GMM can be easily plugged in.},
archivePrefix = {arXiv},
arxivId = {1611.05148},
author = {Jiang, Zhuxi and Zheng, Yin and Tan, Huachun and Tang, Bangsheng and Zhou, Hanning},
eprint = {1611.05148},
file = {:mnt/storage/documents/formacio/UOC/4_TFM/01_references/Variational Deep Embedding_An Unsupervised and Generative Approach to Clustering.pdf:pdf},
journal = {Preprint at arXiv:1611.05148},
keywords = {Machine Learning: Deep Learning,Machine Learning: Machine Learning,Machine Learning: Neural Networks},
month = {nov},
title = {{Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering}},
url = {http://arxiv.org/abs/1611.05148},
year = {2016}
}
@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
doi = {10.48550/arxiv.1312.6114},
eprint = {1312.6114},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Auto-encoding variational bayes}},
url = {https://arxiv.org/abs/1312.6114v10},
year = {2014}
}
@article{Eskandari2020,
author = {Eskandari, Aram},
file = {:mnt/storage/documents/formacio/UOC/4_TFM/01_references/VAE-clustering of neural signals.pdf:pdf},
journal = {Dissertation at http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-273627},
title = {{VAE-clustering of neural signals and their association to cytokines}},
year = {2020}
}
@article{Karim2021,
abstract = {Clustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.},
author = {Karim, Md Rezaul and Beyan, Oya and Zappa, Achille and Costa, Ivan G. and Rebholz-Schuhmann, Dietrich and Cochez, Michael and Decker, Stefan},
doi = {10.1093/bib/bbz170},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karim et al. - 2021 - Deep learning-based clustering approaches for bioinformatics.pdf:pdf},
issn = {14774054},
journal = {Briefings in Bioinformatics},
number = {1},
pages = {393--415},
pmid = {32008043},
title = {{Deep learning-based clustering approaches for bioinformatics}},
volume = {22},
year = {2021}
}
@article{Kingma2019,
abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
archivePrefix = {arXiv},
arxivId = {1906.02691},
author = {Kingma, Diederik P. and Welling, Max},
doi = {10.1561/2200000056},
eprint = {1906.02691},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Welling - 2019 - An introduction to variational autoencoders.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
number = {4},
pages = {307--392},
title = {{An introduction to variational autoencoders}},
volume = {12},
year = {2019}
}
@article{Rappoport2018,
abstract = {Recent high throughput experimental methods have been used to collect large biomedical omics datasets. Clustering of single omic datasets has proven invaluable for biological and medical research. The decreasing cost and development of additional high throughput methods now enable measurement of multi-omic data. Clustering multi-omic data has the potential to reveal further systems-level insights, but raises computational and biological challenges. Here, we review algorithms for multi-omics clustering, and discuss key issues in applying these algorithms. Our review covers methods developed specifically for omic data as well as generic multi-view methods developed in the machine learning community for joint clustering of multiple data types. In addition, using cancer data from TCGA, we perform an extensive benchmark spanning ten different cancer types, providing the first systematic comparison of leading multi-omics and multi-view clustering algorithms. The results highlight key issues regarding the use of single- versus multi-omics, the choice of clustering strategy, the power of generic multi-view methods and the use of approximated p-values for gauging solution quality. Due to the growing use of multi-omics data, we expect these issues to be important for future progress in the field.},
author = {Rappoport, Nimrod and Shamir, Ron},
doi = {10.1093/nar/gky889},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rappoport, Shamir - 2018 - Multi-omic and multi-view clustering algorithms review and cancer benchmark.pdf:pdf},
issn = {0305-1048},
journal = {Nucleic Acids Research},
number = {20},
pages = {10546--10562},
pmid = {30295871},
publisher = {Oxford University Press},
title = {{Multi-omic and multi-view clustering algorithms: review and cancer benchmark}},
volume = {46},
year = {2018}
}
@article{Fortuin2021,
abstract = {Clustering high-dimensional data, such as images or biological measurements, is a longstanding problem and has been studied extensively. Recently, Deep Clustering has gained popularity due to its flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of highdimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the lower dimensional latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on the MNIST benchmark data set and challenging real-world tasks of clustering mouse organs from single-cell RNA-sequencing measurements and defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines as well as competitor methods.},
author = {Fortuin, Vincent and Somnath, Vignesh Ram and Claassen, Manfred},
doi = {10.1371/journal.pcbi.1009086},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fortuin, Somnath, Claassen - 2021 - Mixture-of-Experts Variational Autoencoder for clustering and generating from similaritybased repres.pdf:pdf},
isbn = {1111111111},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {6},
pages = {1--17},
pmid = {34191792},
title = {{Mixture-of-Experts Variational Autoencoder for clustering and generating from similaritybased representations on single cell data Andreas KopfID}},
url = {http://dx.doi.org/10.1371/journal.pcbi.1009086},
volume = {17},
year = {2021}
}
@article{Blekherman2011,
abstract = {It is well known that significant metabolic change take place as cells are transformed from normal to malignant. This review focuses on the use of different bioinformatics tools in cancer metabolomics studies. The article begins by describing different metabolomics technologies and data generation techniques. Overview of the data pre-processing techniques is provided and multivariate data analysis techniques are discussed and illustrated with case studies, including principal component analysis, clustering techniques, self-organizing maps, partial least squares, and discriminant function analysis. Also included is a discussion of available software packages. {\textcopyright} 2011 The Author(s).},
author = {Blekherman, Grigoriy and Laubenbacher, Reinhard and Cortes, Diego F. and Mendes, Pedro and Torti, Frank M. and Akman, Steven and Torti, Suzy V. and Shulaev, Vladimir},
doi = {10.1007/s11306-010-0270-3},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blekherman et al. - 2011 - Bioinformatics tools for cancer metabolomics.pdf:pdf},
issn = {15733882},
journal = {Metabolomics},
keywords = {Bioinformatics,Cancer,Mass spectrometry,Metabolite profiling,Metabolomics,NMR},
month = {sep},
number = {3},
pages = {329--343},
publisher = {Springer},
title = {{Bioinformatics tools for cancer metabolomics}},
url = {https://link.springer.com/article/10.1007/s11306-010-0270-3},
volume = {7},
year = {2011}
}
@incollection{Masood2015,
abstract = {Dealing with data means to group information into a set of categories either in order to learn new artifacts or understand new domains. For this purpose researchers have always looked for the hidden patterns in data that can be defined and compared with other known notions based on the similarity or dissimilarity of their attributes according to well-defined rules. Data mining, having the tools of data classification and data clustering, is one of the most powerful techniques to deal with data in such a manner that it can help researchers identify the required information. As a step forward to address this challenge, experts have utilized clustering techniques as a mean of exploring hidden structure and patterns in underlying data. Improved stability, robustness and accuracy of unsupervised data classification in many fields including pattern recognition, machine learning, information retrieval, image analysis and bioinformatics, clustering has proven itself as a reliable tool. To identify the clusters in datasets algorithm are utilized to partition data set into several groups based on the similarity within a group. There is no specific clustering algorithm, but various algorithms are utilized based on domain of data that constitutes a cluster and the level of efficiency required. Clustering techniques are categorized based upon different approaches. This paper is a survey of few clustering techniques out of many in data mining. For the purpose five of the most common clustering techniques out of many have been discussed. The clustering techniques which have been surveyed are: K-medoids, K-means, Fuzzy C-means, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Self-Organizing Map (SOM) clustering.},
author = {Masood, Muhammad Ali and Khan, M N A and Zulfikar, Shaheed and Bhutto, Ali},
booktitle = {Modern Education and Computer Science},
doi = {10.5815/ijmecs.2015.01.06},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Masood et al. - 2015 - Modern Education and Computer Science.pdf:pdf},
keywords = {DBSCAN,Data Mining,Hierarchical Clustering,Index Terms-Clustering Techniques,Performance Analysis},
pages = {38--46},
title = {{Clustering Techniques in Bioinformatics}},
url = {http://www.mecs-press.org/},
volume = {1},
year = {2015}
}
@article{Makhzani2015,
abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
archivePrefix = {arXiv},
arxivId = {1511.05644},
author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
doi = {10.48550/arxiv.1511.05644},
eprint = {1511.05644},
file = {:home/carles/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Makhzani et al. - 2015 - Adversarial Autoencoders.pdf:pdf},
journal = {Preprint at arXiv:1511.05644},
month = {nov},
title = {{Adversarial Autoencoders}},
url = {https://arxiv.org/abs/1511.05644v2 http://arxiv.org/abs/1511.05644},
year = {2015}
}
