{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare some things\n",
    "## Load some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4fIhSUeUwLYW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from models import get_autoencoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings output (TSNE outputs one very time)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glycine</th>\n",
       "      <th>alanine</th>\n",
       "      <th>valine</th>\n",
       "      <th>leucine</th>\n",
       "      <th>isoleucine</th>\n",
       "      <th>serine</th>\n",
       "      <th>threonine</th>\n",
       "      <th>proline</th>\n",
       "      <th>arginine</th>\n",
       "      <th>glutamic</th>\n",
       "      <th>...</th>\n",
       "      <th>tcc</th>\n",
       "      <th>m_bu_p</th>\n",
       "      <th>mehp</th>\n",
       "      <th>x4_hpb</th>\n",
       "      <th>ibp</th>\n",
       "      <th>cibp</th>\n",
       "      <th>apap</th>\n",
       "      <th>apap_g</th>\n",
       "      <th>dcf</th>\n",
       "      <th>hctz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S000178_1</th>\n",
       "      <td>0.203059</td>\n",
       "      <td>0.111499</td>\n",
       "      <td>0.191536</td>\n",
       "      <td>0.250043</td>\n",
       "      <td>0.257227</td>\n",
       "      <td>0.298541</td>\n",
       "      <td>0.066532</td>\n",
       "      <td>0.136444</td>\n",
       "      <td>0.188422</td>\n",
       "      <td>0.174716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271801</td>\n",
       "      <td>0.266637</td>\n",
       "      <td>0.161973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.028760</td>\n",
       "      <td>0.296658</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S000443_1</th>\n",
       "      <td>0.073279</td>\n",
       "      <td>0.115869</td>\n",
       "      <td>0.132288</td>\n",
       "      <td>0.149303</td>\n",
       "      <td>0.148693</td>\n",
       "      <td>0.084232</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>0.092352</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.074864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.221467</td>\n",
       "      <td>0.197153</td>\n",
       "      <td>0.129863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009566</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.199715</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S000491_1</th>\n",
       "      <td>0.212805</td>\n",
       "      <td>0.304089</td>\n",
       "      <td>0.249373</td>\n",
       "      <td>0.263820</td>\n",
       "      <td>0.277999</td>\n",
       "      <td>0.385945</td>\n",
       "      <td>0.242866</td>\n",
       "      <td>0.262947</td>\n",
       "      <td>0.085272</td>\n",
       "      <td>0.116247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.431987</td>\n",
       "      <td>0.480285</td>\n",
       "      <td>0.612164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030228</td>\n",
       "      <td>0.060776</td>\n",
       "      <td>0.349946</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S000732_1</th>\n",
       "      <td>0.136319</td>\n",
       "      <td>0.715659</td>\n",
       "      <td>0.582759</td>\n",
       "      <td>0.654555</td>\n",
       "      <td>0.671802</td>\n",
       "      <td>0.386421</td>\n",
       "      <td>0.549783</td>\n",
       "      <td>0.517717</td>\n",
       "      <td>0.404796</td>\n",
       "      <td>0.199605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168995</td>\n",
       "      <td>0.150266</td>\n",
       "      <td>0.189479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.348757</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S000732_2</th>\n",
       "      <td>0.178756</td>\n",
       "      <td>0.387277</td>\n",
       "      <td>0.334169</td>\n",
       "      <td>0.344584</td>\n",
       "      <td>0.369569</td>\n",
       "      <td>0.187183</td>\n",
       "      <td>0.546836</td>\n",
       "      <td>0.228796</td>\n",
       "      <td>0.201617</td>\n",
       "      <td>0.165235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123065</td>\n",
       "      <td>0.164727</td>\n",
       "      <td>0.188603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.212806</td>\n",
       "      <td>0.369097</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            glycine   alanine    valine   leucine  isoleucine    serine  \\\n",
       "S000178_1  0.203059  0.111499  0.191536  0.250043    0.257227  0.298541   \n",
       "S000443_1  0.073279  0.115869  0.132288  0.149303    0.148693  0.084232   \n",
       "S000491_1  0.212805  0.304089  0.249373  0.263820    0.277999  0.385945   \n",
       "S000732_1  0.136319  0.715659  0.582759  0.654555    0.671802  0.386421   \n",
       "S000732_2  0.178756  0.387277  0.334169  0.344584    0.369569  0.187183   \n",
       "\n",
       "           threonine   proline  arginine  glutamic  ...  tcc    m_bu_p  \\\n",
       "S000178_1   0.066532  0.136444  0.188422  0.174716  ...  0.0  0.271801   \n",
       "S000443_1   0.022798  0.092352  0.025114  0.074864  ...  0.0  0.221467   \n",
       "S000491_1   0.242866  0.262947  0.085272  0.116247  ...  0.0  0.431987   \n",
       "S000732_1   0.549783  0.517717  0.404796  0.199605  ...  0.0  0.168995   \n",
       "S000732_2   0.546836  0.228796  0.201617  0.165235  ...  0.0  0.123065   \n",
       "\n",
       "               mehp    x4_hpb  ibp  cibp      apap    apap_g       dcf  hctz  \n",
       "S000178_1  0.266637  0.161973  0.0   0.0  0.018271  0.028760  0.296658   0.0  \n",
       "S000443_1  0.197153  0.129863  0.0   0.0  0.009566  0.010220  0.199715   0.0  \n",
       "S000491_1  0.480285  0.612164  0.0   0.0  0.030228  0.060776  0.349946   0.0  \n",
       "S000732_1  0.150266  0.189479  0.0   0.0  0.001626  0.004522  0.348757   0.0  \n",
       "S000732_2  0.164727  0.188603  0.0   0.0  0.081787  0.212806  0.369097   0.0  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"../DO_NOT_PUBLISH/MetaboData_pseudoID.xlsx\"\n",
    "metabol = pd.read_excel(data_file, sheet_name=\"metabs\", index_col=0).T\n",
    "# Min-max normalization\n",
    "metabol.iloc[:,:] = MinMaxScaler().fit_transform(metabol)\n",
    "metabol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X.SampleID</th>\n",
       "      <th>KKHNGID</th>\n",
       "      <th>time_origin</th>\n",
       "      <th>gender</th>\n",
       "      <th>pred_risk</th>\n",
       "      <th>CVrisk3</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SCBIA1</th>\n",
       "      <th>SCANT2</th>\n",
       "      <th>SCBIA4</th>\n",
       "      <th>TG</th>\n",
       "      <th>HDL</th>\n",
       "      <th>LDL</th>\n",
       "      <th>CHO</th>\n",
       "      <th>HBA1CPC</th>\n",
       "      <th>SCBP1</th>\n",
       "      <th>SCBP2</th>\n",
       "      <th>HSCRP</th>\n",
       "      <th>CR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S000178_1</th>\n",
       "      <td>S000178_1</td>\n",
       "      <td>20161</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>22.52</td>\n",
       "      <td>90.0</td>\n",
       "      <td>10.15</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.68</td>\n",
       "      <td>4.94</td>\n",
       "      <td>6.87</td>\n",
       "      <td>5.1</td>\n",
       "      <td>125</td>\n",
       "      <td>90</td>\n",
       "      <td>0.71</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S000443_1</th>\n",
       "      <td>S000443_1</td>\n",
       "      <td>3557</td>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>23.06</td>\n",
       "      <td>96.8</td>\n",
       "      <td>33.33</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.78</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.5</td>\n",
       "      <td>113</td>\n",
       "      <td>78</td>\n",
       "      <td>0.32</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S000491_1</th>\n",
       "      <td>S000491_1</td>\n",
       "      <td>1783</td>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>22.15</td>\n",
       "      <td>92.3</td>\n",
       "      <td>30.20</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.69</td>\n",
       "      <td>5.3</td>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S000732_1</th>\n",
       "      <td>S000732_1</td>\n",
       "      <td>104735</td>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>22.90</td>\n",
       "      <td>87.6</td>\n",
       "      <td>30.42</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.56</td>\n",
       "      <td>3.54</td>\n",
       "      <td>5.2</td>\n",
       "      <td>109</td>\n",
       "      <td>83</td>\n",
       "      <td>2.86</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S000732_2</th>\n",
       "      <td>S000732_2</td>\n",
       "      <td>104735</td>\n",
       "      <td>six</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>23.34</td>\n",
       "      <td>102.5</td>\n",
       "      <td>31.41</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.82</td>\n",
       "      <td>4.9</td>\n",
       "      <td>117</td>\n",
       "      <td>83</td>\n",
       "      <td>2.13</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X.SampleID  KKHNGID time_origin  gender  pred_risk  CVrisk3  AGE  \\\n",
       "S000178_1  S000178_1    20161    baseline       0   0.020379        2   51   \n",
       "S000443_1  S000443_1     3557    baseline       1   0.000580        0   41   \n",
       "S000491_1  S000491_1     1783    baseline       1   0.001671        1   47   \n",
       "S000732_1  S000732_1   104735    baseline       1   0.000308        0   38   \n",
       "S000732_2  S000732_2   104735         six       1   0.000398        0   39   \n",
       "\n",
       "           SCBIA1  SCANT2  SCBIA4    TG   HDL   LDL   CHO  HBA1CPC  SCBP1  \\\n",
       "S000178_1   22.52    90.0   10.15  1.13  1.68  4.94  6.87      5.1    125   \n",
       "S000443_1   23.06    96.8   33.33  1.30  1.68  2.78  4.29      5.5    113   \n",
       "S000491_1   22.15    92.3   30.20  0.90  1.46  2.80  4.69      5.3    117   \n",
       "S000732_1   22.90    87.6   30.42  1.38  1.94  1.56  3.54      5.2    109   \n",
       "S000732_2   23.34   102.5   31.41  1.10  1.86  1.65  3.82      4.9    117   \n",
       "\n",
       "           SCBP2  HSCRP    CR  \n",
       "S000178_1     90   0.71  77.0  \n",
       "S000443_1     78   0.32  63.0  \n",
       "S000491_1     82   0.78  65.0  \n",
       "S000732_1     83   2.86  70.0  \n",
       "S000732_2     83   2.13  78.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = pd.read_excel(data_file, sheet_name=\"samples\", index_col=0)\n",
    "\n",
    "# Drop the observations cointaining missing values:\n",
    "drop_index = samples[samples['CVrisk3'].isna()].index\n",
    "metabol.drop(drop_index, inplace=True, errors=\"ignore\")\n",
    "# Subset only the indeces present in the metabol data set\n",
    "samples = samples.loc[metabol.index,:]\n",
    "\n",
    "# Code gender as 0, 1\n",
    "samples['gender'] = samples['gender'] -1\n",
    "# Code CVrisk3 as 0, 1, 2\n",
    "samples['CVrisk3'] = pd.Categorical(samples['CVrisk3'], categories=('Low','Med','Hig')).codes\n",
    "# Convert time_origin to categorical type\n",
    "samples['time_origin'] = pd.Categorical(samples['time_origin'], categories=('baseline','six','twelve'))\n",
    "\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 1020\n",
      "Number of variables: 411\n",
      "Number of data points: 419220\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of observations:\", metabol.shape[0])\n",
    "print(\"Number of variables:\", metabol.shape[1])\n",
    "print(\"Number of data points:\", np.multiply(*metabol.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the health variabes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try AE with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1\n",
    "MOMENTUM = 0.8\n",
    "optimizer = keras.optimizers.SGD(learning_rate=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "INPUT_DIM = metabol.shape[1]\n",
    "LATENT_DIM = 5\n",
    "N_CLUSTERS = LATENT_DIM\n",
    "\n",
    "num_data_points = np.multiply(*metabol.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate layers number and dimentions\n",
    "\n",
    "Let's try different numbers and dimentions of intermediate layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 2048) [data/params ratio: 0.1] [loss: 0.0099, val_loss: 0.0100, ratio: 0.9941]\n",
      "(64, 32) [data/params ratio: 7.3] [loss: 0.0101, val_loss: 0.0101, ratio: 0.9934]\n",
      "(32, 16) [data/params ratio: 15.0] [loss: 0.0103, val_loss: 0.0103, ratio: 0.9940]\n",
      "(16, 4) [data/params ratio: 30.4] [loss: 0.0121, val_loss: 0.0119, ratio: 1.0191]\n",
      "(16, 16, 64) [data/params ratio: 24.7] [loss: 0.0122, val_loss: 0.0119, ratio: 1.0213]\n",
      "(16, 16, 128) [data/params ratio: 21.2] [loss: 0.0116, val_loss: 0.0114, ratio: 1.0104]\n",
      "(16, 16, 256) [data/params ratio: 16.5] [loss: 0.0123, val_loss: 0.0120, ratio: 1.0205]\n",
      "(32, 32, 64) [data/params ratio: 12.4] [loss: 0.0112, val_loss: 0.0111, ratio: 1.0072]\n",
      "(32, 32, 128) [data/params ratio: 10.9] [loss: 0.0110, val_loss: 0.0109, ratio: 1.0072]\n",
      "(32, 32, 256) [data/params ratio: 8.7] [loss: 0.0113, val_loss: 0.0112, ratio: 1.0081]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "INTERMEDIATE_DIMS = [(512, 512, 2048), # same as in MNIST, for reference\n",
    "                     (64, 32),\n",
    "                     (32, 16),\n",
    "                     (16, 4),\n",
    "                     (16, 16, 64),\n",
    "                     (16, 16, 128),\n",
    "                     (16, 16, 256),\n",
    "                     (32, 32, 64),\n",
    "                     (32, 32, 128),\n",
    "                     (32, 32, 256)\n",
    "                    ]\n",
    "\n",
    "# Try every combination of dimention 5 times, \n",
    "# then get the mean of the results\n",
    "for dims in INTERMEDIATE_DIMS:\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    loss_ratio = []\n",
    "    for i in np.arange(5):\n",
    "        model_ae = get_autoencoder_model(INPUT_DIM, LATENT_DIM, dims)\n",
    "        data_params_ratio = num_data_points / (model_ae.encoder.count_params() + \n",
    "                                               model_ae.decoder.count_params())\n",
    "        if i == 0:\n",
    "            print(f'{dims} [data/params ratio: {data_params_ratio:.1f}]', end=\" \")\n",
    "\n",
    "        model_ae.compile(optimizer=optimizer, loss=\"mse\")\n",
    "        history = model_ae.fit(metabol, metabol,\n",
    "                               epochs=EPOCHS,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               validation_split=0.2,\n",
    "                               verbose=0)\n",
    "        loss.append(history.history['loss'][-1])\n",
    "        val_loss.append(history.history['val_loss'][-1])\n",
    "        loss_ratio.append(loss[-1] / val_loss[-1])\n",
    "\n",
    "    print(f\"[loss: {np.mean(loss):.4f}, val_loss: {np.mean(val_loss):.4f}, ratio: {np.mean(loss_ratio):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best loss is achieved with only two intermediate layers (64,32), but the data/params ratio is very low and this can make the model easily overfit.\n",
    "\n",
    "There are three configurations that get better a data/params ratio and achieve a similar loss:\n",
    "- (32, 16)\n",
    "- (16, 16, 64)\n",
    "- (16, 16, 128)\n",
    "\n",
    "The difference in loss can be minimized by training for more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16) [data/params ratio: 15.0] [loss: 0.0094, val_loss: 0.0095, ratio: 0.9916]\n",
      "(16, 16, 64) [data/params ratio: 24.7] [loss: 0.0096, val_loss: 0.0096, ratio: 0.9968]\n",
      "(16, 16, 128) [data/params ratio: 21.2] [loss: 0.0095, val_loss: 0.0096, ratio: 0.9939]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "INTERMEDIATE_DIMS = [(32, 16),\n",
    "                     (16, 16, 64),\n",
    "                     (16, 16, 128),\n",
    "                    ]\n",
    "\n",
    "# Try every combination of dimention 5 times, \n",
    "# then get the mean of the results\n",
    "for dims in INTERMEDIATE_DIMS:\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    loss_ratio = []\n",
    "    for i in np.arange(5):\n",
    "        model_ae = get_autoencoder_model(INPUT_DIM, LATENT_DIM, dims)\n",
    "        data_params_ratio = num_data_points / (model_ae.encoder.count_params() + \n",
    "                                               model_ae.decoder.count_params())\n",
    "        if i == 0:\n",
    "            print(f'{dims} [data/params ratio: {data_params_ratio:.1f}]', end=\" \")\n",
    "\n",
    "        model_ae.compile(optimizer=optimizer, loss=\"mse\")\n",
    "        history = model_ae.fit(metabol, metabol,\n",
    "                               epochs=EPOCHS,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               validation_split=0.2,\n",
    "                               verbose=0)\n",
    "        loss.append(history.history['loss'][-1])\n",
    "        val_loss.append(history.history['val_loss'][-1])\n",
    "        loss_ratio.append(loss[-1] / val_loss[-1])\n",
    "\n",
    "    print(f\"[loss: {np.mean(loss):.4f}, val_loss: {np.mean(val_loss):.4f}, ratio: {np.mean(loss_ratio):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll select the combination (16, 16, 64), since it achieves the better relation between data/params ratio and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "\n",
    "Let's now try different batch sizes to see if there is are any big differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 16 [loss: 0.0102, val_loss: 0.0102, ratio: 0.9980]\n",
      "batch size: 32 [loss: 0.0120, val_loss: 0.0118, ratio: 1.0187]\n",
      "batch size: 64 [loss: 0.0125, val_loss: 0.0122, ratio: 1.0258]\n",
      "batch size: 128 [loss: 0.0127, val_loss: 0.0124, ratio: 1.0264]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "INTERMEDIATE_DIMS = (16, 16, 64)\n",
    "BATCH_SIZES = (16, 32, 64, 128)\n",
    "\n",
    "# Try every batch size 5 times, \n",
    "# then get the mean of the results\n",
    "for batch_size in BATCH_SIZES:\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    loss_ratio = []\n",
    "    print(f'batch size: {batch_size}', end=\" \")\n",
    "    for i in np.arange(5):\n",
    "        model_ae = get_autoencoder_model(INPUT_DIM, LATENT_DIM, INTERMEDIATE_DIMS)\n",
    "        model_ae.compile(optimizer=optimizer, loss=\"mse\")\n",
    "        history = model_ae.fit(metabol, metabol,\n",
    "                               epochs=EPOCHS,\n",
    "                               batch_size=batch_size,\n",
    "                               validation_split=0.2,\n",
    "                               verbose=0)\n",
    "        loss.append(history.history['loss'][-1])\n",
    "        val_loss.append(history.history['val_loss'][-1])\n",
    "        loss_ratio.append(loss[-1] / val_loss[-1])\n",
    "\n",
    "    print(f\"[loss: {np.mean(loss):.4f}, val_loss: {np.mean(val_loss):.4f}, ratio: {np.mean(loss_ratio):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller batch sizes achieve a lower loss, but also the training process takes longer.\n",
    "\n",
    "Both things can be contrarrested by selecting a different amount of epochs. So it does'nt seem to be criticall.\n",
    "\n",
    "I'll select a batch size of 32 and vary the number of epochs depending on how long it takes for the loss to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, I decided to select the following parameters:\n",
    "- Intermediate dimentions: (16, 16, 64) *\n",
    "- Batch size: 32\n",
    "\n",
    "*\\* Finally, I found that the deep clustering models performed better with **two intermadiate layers**, with dimentions **(16, 32)** .*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data / params ratio: 27.93\n"
     ]
    }
   ],
   "source": [
    "model_ae = get_autoencoder_model(INPUT_DIM, LATENT_DIM, (16, 32))\n",
    "data_params_ratio = num_data_points / (model_ae.encoder.count_params() + \n",
    "                                       model_ae.decoder.count_params())\n",
    "print(f\"Data / params ratio: {data_params_ratio:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
